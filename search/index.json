[{"content":"對於大多數系統，記憶體之間沒有什麼差別，每個記憶體分頁都和其他分頁一樣好，但是在 NUMA 系統中，記憶體和目前正在執行的 process 之間的距離是需要考慮的，如果一個 process 不斷的存取遠端節點上的頁面，則很有可能導致效能大幅下降。對於 NUMA 系統需要特別考慮本地記憶體，因為他具有最小延遲以及最佳頻寬的特性。\nkernel 有個很重要的任務便是為 process 分配記憶體，使得該 process 能夠以最高效能執行，此外，kernel 有許多機制可以用來確認節點的位置，目的是為了確保以最小化 NUMA 距離的方式分配記憶體，在執行 process 期間，locality 可能會受到許多因素影響，如 scheduler, memory allocation 等等，因此 kernel 需要能夠在 process 執行時移動記憶體頁面，這稱為 [[page migration (頁面遷移)]]\n關於記憶體分配也可以通過 process 本身建立記憶體的策略進行控制，例如 process 可以知道使用的記憶體位於哪一個節點等等。\n[[NUMA]] NUMA 為非一致性記憶體存取，隨著處理器數量增加，NUMA 技術越來越重要，因為處理器數量增加對於構建 UMA 系統會變得越來越困難，NUMA 系統與一般具有多處理器的常規系統差別在於一些記憶體是屬於處理器的本地記憶體，可以有最小的延遲時間和最高 throughput，而其他記憶體屬於遠端的，性能上不如本地記憶體，延遲更大，更小的 throughput。\n下面將會介紹在沒有 NUMA 情況下記憶體管理是如何運作的，然而看到記憶體管理機制在考慮 NUMA 的情況做出了哪一些調整，接著是高效率分配記憶體會遇到的問題，然後討論本地記憶體回收以及記憶體遷移的問題。\n最後看到 Linux process 如何手動控制 NUMA 系統中記憶體分配，使用的方法首先是 memory policies，以及 cpusets。\nExample of NUMA system 對於 SMP (Symmetric Multi-Processor) 以及 UP (Single Processor) 系統來說，記憶體是統一的，也就是存取所有記憶體有相同的成本。在 NUMA 系統中會因為 NUMA 距離而有所差異，優化 NUMA 系統上執行的應用程式本質上我們希望以最佳效能的放置應用程式使用到的資料結構等等。\n關於 NUMA 距離 (NUMA distance)，kernel 會藉由 ACPI (Advanced Configuration and Power Interface) 以及 SLIT (System Locality Information Table) 來獲得。根據 ACPI 的標準，SLIT 表中用於常規 SMP 模式的記憶體存取和本地 NUMA 距離的值被標準化為 10，也就是本地記憶體存取定義為 10，其他距離會根據這個相對值來決定，例如跨節點的 NUMA 存取距離會比 10 還要大。\n上面這是一張 NUMA 模型圖，灰色為一個 NUMA 節點，這一些 NUMA 節點藉由 NUMA Interconnect 來連結。每一個 NUMA 節點有兩個 processors，以及一個 local memory 和 cachelines\n假設如圖所示，有一個 process 在 Node 2 上面的 4 號 CPU 上面執行，如果我們存取本地記憶體，則 NUMA 距離為 10。該 process 在 Node 2 上面執行，因此放在 Node 2 上的記憶體對於我們來說就是本地記憶體，距離為 10，這是可能的最小 NUMA 距離。\n如果 CPU 4 上執行的 process 要存取 Node1 上面的記憶體，則需要通過 NUMA interlink 進行存取，並且需要將結果移動回到本地，這比起存取本地節點上的記憶體來慢的許多。假設存取 Node1 上面記憶體的 NUMA 距離為 14，表示[[遠端存取]] 是本地存取的 1.4 倍。NUMA 距離是抽象概念，會隨著硬體技術和 NUMA 互連速度有所改變，較快互連速度可降低遠程存取的延遲。\n在使用 NUMA 節點時也需要特別注意該節點外聯的 I/O 設備，例如在 Node1 上面執行的 Process 不需要經過 NUMA interlink 來存取外部設備，同樣的 Node4 也有差不多的狀況。\n關於 Linux Mememory Management 上面是簡單的 Linux Memory Management 的概念圖，核心部分為為 page allocator，page allocator 以 page 為大小分配一塊又一塊的記憶體。\n通常 page 大小為 4KB，在 Itanium 上面可能為 16k。所有其他記憶體分配器都是某種程度上基於 page allocator 的，並且從 page allocator 管理的 pool 中取出頁面，頁面可以映射到 process 的 virtual memory space 中，通常映射到 userspace 的 page 有兩種使用方式\n一種是用於 anon page 或是說 [[anon memory]]，不和檔案關聯，通常用於變數，stack, heap 等等，anon page 比較輕量，通常比 file backed pages 還要高效，因為不需要映射到記憶體 (映射到記憶體的頁面需要經過序列化才可以存取)。如果記憶體不足時，可以將 anon page swap 出去，不過這會增加未來處理該頁面所需要付出的開銷 另外為 page cache 或是 buffer, 通常這一些會儲存在一些輔助儲存媒介上，如硬碟等等，如果記憶體面臨壓力或是資源不足時，則 page cache 可能被移除，因為 page cache 可以之後通過硬碟進行恢復。page cache 包含 process 中的可執行程式碼，可執行程式碼可以通過 mmap 映射到其他 memory space 中。 kernel 本身也需要一些 page 來儲存一些 meta data，如用於 fs 的緩衝區，此外 kernel 還需要為不同大小的 struct 分配記憶體，這些結構使用的單位可能不是以 page 為大小。為此，有了 slab 分配器，slab 分配器從頁面分配器中查詢單個 page 或是連續的 page，接著運用自己的內部邏輯，根據 kernel 或是 driver 分配不同大小的記憶體區塊。 slab 分配器使用了許多高校緩衝技術，對於小物件的記憶體分配來說，性能非常的好。slab 主要用在檔案被開啟的相關訊息，或是最近使用的檔案名稱，把這一些資訊緩衝起來加速 fs 查詢操作，或是其他如 kernel 中用的 lock, 某些物件的狀態等等。\ndevice driver 會利用 page allocator 以及 slab allocator 來分配記憶體和管理 device，device driver 根據所需要的 page sized 還有其他不同的變體，如 vmalloc，可以分配比較大的記憶體 chunks，這些記憶體塊在 kernel context 中看起來似乎連續，但實際上在物理上並不是連續的。vmalloc 可以為大塊記憶體產生出幾乎連續的記憶體。vmalloc 即便遇到系統碎片化等問題，如果 page allocator 無法提供物理上連續的記憶體，這時候就可以使用 vmalloc 處理。vmalloc 在 kernel space 上記憶體連續，但是在物理記憶體地址上不是連續的。但 vmalloc 也有其缺點，通常需要通過 page table 進行轉換，與直接從 page allocator 或的物理地址相比，這樣轉換多了 overhead，且 vmalloc 只能用在 kernel 中，無法映射到 userspace 的程式。\n對於 PCI 設備，通常會使用 dma_alloc_coherent 函數，這是專門用於 DMA 操作的記憶體，DMA 記憶體有幾個特點，例如一致性，DMA 記憶體對 CPU 和 PCI 等設備保持一致性，這表示不需要額外的 buffer 或是 cache 用來同步操作。不同硬體平台對於 DMA 記憶體要求可能˙不同，如記憶體對齊，記憶體地址範圍或是映射方式，dma_alloc_coherent 獲得的記憶體很大程度依賴於底層硬體實現。有些硬體可能支援使用 Huge Page 提升 DMA 效能等等\n以下為關於 Linux 記憶體分配的相關 API\n關於 NUMA 記憶體 在 NUMA 系統中每個節點的記憶體是單獨管理的，每個節點都存在於 page pools 中。每個節點都與 swapper thread 所關聯，用來處理記憶體回收。節點中的每個記憶體地址範圍稱為分配區域。\n在 Linux 中有一個機制稱為 zone structure, 裡面包含 free list 存放可以使用的記憶體分頁，zone 結構是 Linux kernel 用來組織以及管理物理記憶體的一種結構\nZONE_DMA: 低記憶體地址記憶體，用來支援需要特定記憶體區域的 DMA device ZONE_NORMAL: 普通記憶體區域，用於 kernel 和 應用程式的記憶體分配 ZONE_HIGHMEM: 主要用於 32 bit 系統 而回收策略部分，使用 active list 以及 inactive list 管理回收機制，page 會在這兩個 list 之間移動以確認哪一些 page 可以回收，如果存取位於 inactive list 裡面的 page，則該 page 會移動到 active list。swapper 會將 page 從 active list 移動到 inactive list，如果遇到記憶體資源吃緊時，會從 inactive list 末端開始回收 page。 如果我們在 NUMA 上發出請求分配記憶體的請求，我們將需要決定從哪一個節點上的哪個 pool 去得到記憶體。如果沒有 cpuset 限制，NUMA 系統會嘗試將記憶體分配到 process 正在執行的節點上，原因為新分配的記憶體通常在分配之後很常被使用，這個結論可以從 [[TPP Transparent Page Placement for CXL-Enabled Tiered-Memory]] 得知，因為他是 hot 的，把他放在本地記憶體是合理的。\n但有可能我們需要的記憶體超過目前節點所能夠提供的數量，我們需要某一些機制讓我們能夠指定記憶體應該分配在哪一些節點，避免單一節點過載等等。或是一些特殊場景，如 driver 開發者已經知道某個資料結構不會那麼常被存取，或是其他節點會更頻繁的存取該結構，則分配在本地節點就不會是最佳解了。為了應對更佳玲羅的需求，NUMA 中會有帶有 _node 後綴的函數，為 NUMA 系統使用的分配器，例如 kmalloc_node() 可以直接分配記憶體在目標節點上。以下表格整理了 NUMA 相關記憶體分配器\n最佳應用 NUMA 的應用程式特徵 只利用單一節點資源的小型應用程式可以利用本地節點的所有元素來實現最佳速度執行。這個應用程式如果使用的記憶體不超過單一節點可以提供的記憶體量，且使用的 CPU 數量不超過一個節點可以提供的 CPU 數量。\n關於多節點應用程式 如果本地節點的資源不足以供應給一個應用程式使用，則我們勢必需要存取到非本地節點的資源，對於應用程式中的資源，大致上可以分成 Thread specific data, Shared read/write data, Shared read/only data 這三種，這三種有以下幾種最佳分配記憶體的方式可以參考\nThread Specific Data: 當資料只被本地處理器存取時，可以把它視為本地的資料，因為 Thread Specific Data 不需要跨節點進行存取 Shared Read/Write: 如果某些資料被所有節點以相同的機率存取，那麼最好的方法就是把資料均勻地分布在所有節點上。這種分配策略通常會通過 interleave memory 或是 memory spreading 實現 Shared Read/Only: 當資料是高頻率的 Read/Only 時，最好的策略是讓每一個節點都有一份自己的拷貝，這樣每一個節點只要從自己本地節點讀取就可以，省去了跨節點存取的開銷。但需要注意 Linux 不支援直接進行資料的自動複製管理，而是使用 interleave memory 或是 memory spreading 來處理所有 Shared Data。如果應用程式需要這樣的功能，通常需要自己實現 關於 Linux 中 Interleave Memory, Memory Spreading Interleave Memory (記憶體交錯): 概念上是將記憶體以 block 的形式平均分布在 NUMA 節點上，每一個 memory block 由不同節點負責存取，從而實現存取上的負載平衡。當多處理器需要頻繁的存取一段記憶體時，使用這個方法可以降低單一節點記憶體控制器的壓力。\n在 Linux 中 NUMA 策略中的 interleave 模式可以使用 numactl 工具來設定，詳細 usage 為以下\n1 numactl --interleave=all ./program --interleave=all 指定記憶體將均勻分布在所有 NUMA 節點上\n在 Kernel 的部分，我們可以使用 alloc_pages_interleave() 來使用 Interleave Memory 的策略，或是 mbind_range 等等，詳細可以參考 kernel source 或是 libC 裡面的 numa.h\nMemory Spreading: 概念上是將記憶體分散到多個節點，但是不會像是 Interleave Memory 那樣以固定交錯模式分布。分散的記憶體分布可以考慮更加靈活的條件，例如節點目前的負載程度等等。可以應用在避免節點資源耗盡，保持系統整理記憶體使用均衡，概念有點像是 Interleave Memory。\n在 Linux 中通過 autoNUMA 實現 Memory Spreading，這通常不需要顯式設定。\n如果要手動設定，可以使用以下方式進行設定\n1 numactl --preferred=0 ./program 在上面案例中，優先從節點 0 分配。當節點 0 無法滿足需求時，其他節點的記憶體會作為備用。\nkernel 裡面 NUMA 平衡機制有位於 mm/page_alloc.c 裡面的 find_next_best_node\n對於 NUMA 控制，libC 也有提供一些介面給應用程式使用，如 numa.h 裡面的 numa_set_interleave_mask，或是使用 numactl 這個工具進行控制。而與 kernel 相關 NUMA 控制有 cat /sys/devices/system/node/node*/meminfo。對於 NUMA 平衡，有 echo 1 \u0026gt; /proc/sys/kernel/numa_balancing\n關於 page cache 當 Linux 從硬碟裡面讀取資料時，這些 page 會被 cache 到記憶體裡面，而這部分的記憶體就被稱作 page cache。page cache 主要是用來提高系統效能，減少硬碟 I/O 操作的次數。page cache 不只是用來做 cache，還有以下用途\nExecutable Pages: 當程式需要執行某些從硬碟載入的二進位檔案時，這些頁面可能被映射成可執行頁面，也就是從硬碟讀取資料時，這些 page 被 cache 到記憶體裡面時，page 本身具有執行權限 Memory Mapped Pages: 某些程式可能會使用 memory-mapped-files，也就是把檔案映射到記憶體，從而實現檔案到記憶體之間的對應，可以直接使用記憶體相關 API 去處理檔案，通常這種方式會用於處理大檔案或是共享檔案。 page cache 中的某個 page 被映射到某個 process images 之類，即便該 process 被 kill 了，這一些 page 也不會被立即釋放，這一點和 [[anon memory]] 有所不同，也就是 page cache 具有某部分的持久性 當一個 process 結束之後，page cache 會繼續留在記憶體 這樣的設計是基於空間局部性的考慮，也就是未來可能有其他 process 會需要相同的 page，這些 page 和硬碟的資料有關，基於這樣的假設，之後的 process 就可以減少對於硬碟的 I/O，直接從 page 拿資料，提高系統效能。舉例來說，如果有另外一個 process 需要存取相同的檔案內容，process 可以直接從 page 撈資料。但是當記憶體壓力大時，這一些 page 可能被替換掉，可能是基於 LRU 或是其他策略 關於記憶體回收 以下為 UP 系統以及 SMP 系統簡單的記憶體映射情況\n由 process 映射的頁面是 anon page 和 page cache page。當 process 被 kill 時且已經映射的 page cache page 變成未映射的 page cache pages 時，記憶體裡面隨著時間推時會有越來越多的未映射 page cache pages，也就是這一些 page cache 不屬於任何 process，導致 Free memory 減少。\n當 Free memory 低於作業系統限制時，swapper 開始回收記憶體，如果壓力比較低，且 swapper 運作的頻率並沒有那麼頻繁，則只會釋放 inactive list 裡面的未映射頁面，這樣可以提高 Free memory，且累積的未映射快取頁面可以重新的被利用。\n如果記憶體壓力增加，swapper 可能會更加激進，例如取消映射到 process 的頁面，或是把 [[anon memory]] 交換到 swap space 中。\n關於 NUMA 的記憶體回收 在 NUMA 系統中，很少發生全域回收，也就是針對於所有節點的回收機制。因為要觸發全域回收的前提是所有節點記憶體都耗盡，才會觸發 swapper。\n因此當 Node1 上記憶體耗盡時，標準的 swapper 是不會運作的，因為其他兩個 Node 還有大量的 Free Memory。假設 Node0 沒有足夠的記憶體，可能在 Node0 上面執行的 process 所使用的 [[anon memory]] 等等會在其他節點上，如 Node1。這樣會造成存取延遲。而 NUMA 隨著時間推進，也會面臨到大量記憶體被 unmapped page cache page 所佔用，因此到最後可能會觸發全域回收，但全域回收會影響效能。\n對於大型 NUMA 系統，有更多的節點和更大的記憶體，觸發全域回收的頻率會更小，但是大型 NUMA 系統更多節點表示有更長的存取路徑，導致更大的 NUMA 延遲。過去為了快速釋放記憶體，會直接全域回收 unmapped page cache page，雖然可以馬上獲得大量的 Free Memory，但是對其他節點會造成不良影響，因為他們可能需要重新從硬碟讀取剛剛被清除了 page (這裡連接到剛剛的空間局部性，雖然是未映射，但是之後 process 可能還是會用到該 page，減少 I/O，但是回收之後這樣就失去 page cache 的意義了 )，而且假設硬碟只有連接到一個 Node，對於其他 Node 復原頁面的開銷會變得十分的巨大，且對 I/O 以及直接連接到 I/O 的 Node 帶來非常大的負擔。\n對於 Linux，引入了區域回收試圖解決上面這一些問題。當目前 Node 局部記憶體不足，系統會嘗試輕量回收，也就是只釋放未映射的 page cache page，不去影響 anon page 或 active page。區域回收會優先在本地的 Node 做，除非無法滿足記憶體需求時，才會考慮去對其他非本地記憶體做操作。\n回收本地的 page cache page 是有道理的，page cache page 考慮的是接下來 process 可能會考慮空間局部性用到差不多的檔案，以達到減少 I/O 的作用，但是當記憶體不足時，連 process 都無法載入了，因此優先將 page cache page 釋放，滿足在本地載入 process 的需求，且讓 [[anon memory]] 等等內容都留在本地。\n關於 Linux 的區域回收機制，可以參考以下 /proc/sys/vm\n/proc/sys/vm/zone_reclaim_mode: 用來控制是否啟用區域回收，區域回收就如同上面所說，用於嘗試在本地釋放 unmapped page cache page。當 NUMA 距離不大，存取延遲不明顯時，系統預設關閉 zone_reclaim_mode。如果延遲較高，則會啟用，設置為 1 /proc/sys/vm/zone_reclaim_interval: 用來控制如果沒有成功回收本地的記憶體時，暫停進一步回收的時間，時間單位為秒，也就是暫停進一步掃描本地節點中可回收的記憶體，並在給定時間之後再繼續嘗試 關於 NUMA Interlink 的限制 最上面架構我們可以看到 NUMA 架構中，節點之間的記憶體存取可以通過 NUMA interlink 完成，但是這個機制會有以下限制\n[[cacheline]] 限制: 每個 NUMA 節點只能 cache 有限數量的來自於其他節點的 [[cacheline]]，如果超過這個限制，[[cacheline]] 會被頻繁的替換，導致需要反覆從遠端節點獲得資料，增加存取延遲 請求處理能力限制: 每個節點能夠處理跨節點的需求是有限的，當需求過多時，會導致延遲大幅增長，變成瓶頸 NUMA interlink 本身頻寬有限，當多個 process 同時存取遠端記憶體時，可能會出現競爭，進而減少了每一個請求的 thoughtput，增加延遲 CPUset 控制 [[page migration (頁面遷移)]] cpuset 用於將 process 與特定的 CPU 和記憶體節點綁定，cpuset 包含多個檔案，如 mems, tasks, memory_migrate, mems 定義該 cpuset 可以存取的記憶體節點集合, tasks 列出該 cpuset 中的 process id。對於遷移整個程式的 page 概念上為把該程式的所有 process 都加入到同一個 cpuset 中，接著修改 mems 檔案中記憶體節點集合，如果 memory_migrate 是啟用狀態，則頁面會被自動遷移到新定義的節點。如果是要遷移單個 process 的 page，則將 process 的 pid 寫入到 tasks 檔案中，將該 process 移動到新的 cpuset 中。kernel 會自動進行遷移\n對於 page migration 除了使用 CPUset 控制以外，也可以使用 migratepages 這個套件，這是 numa_migrate_pages syscall 的前端。\n更多關於 CPUset cpuset 為 cgroup 的子系統，用於將系統中多個節點劃分成節點集合，從而為應用程式提供獨立執行環境\n資源隔離: 每個 cpuset 可以限制 cpu 和記憶體限制範圍，應用程式在一個 cpuset 執行，其記憶體分配就被限制在 cpuset 所管轄的節點內，通過這樣的方式，實現 process 存取本地資源 應用: 可以應用於多節點系統上如 NUMA 上面執行多個大型應用程式，並對資源獨立管理 cpuset 提供了記憶體分配控制的功能，如 memory spreading 以及 [[page migration (頁面遷移)]] memory spreading: 如果啟用記憶體分散功能，如 memory_spread_page 或是 memory_spread_slab，記憶體分配會使用類似於 round robin 的方式，在 cpuset 範圍內的所有節點分配記憶體 如果所有應用程式都位於同一個 cpuset，則整個應用程式可以作為一個單位進行 page migration，可以應用於 NUMA 節點資源的管理，或是對於某些應用程式無法感知 NUMA，無法自行優化其記憶體分配模式，就可以使用此方式降低單一節點記憶體過載問題。 至於要不要分散記憶體分部，可參考上面說到的應用程式中常見的資料模式，如 read/only, write/only 等等\n需要面臨的問題 cpuset 和子系統交互可能有問題，如對於 slab 分配器，會使用 array 等去維護自己記憶體的 locality，當 cpu 限制分配節點時，可能導致 slab 沒辦法按照其原本設計從特定節點分配記憶體 當 slab 釋放 cache，如 icache 以及 dentry，通常需要釋放大量物件才能夠回收足夠頁面，具體問題描述如下 slab 分配器為 kernel 中分配機制，用於管理小型，頻繁分配和釋放，短生命週期的物件。他將記憶體分割成固定物件大小的 page 或 cache，如 icache, dentry。slab 會根據每個 page 儲存的物件數量決定什麼時候釋放記憶體，當 cache page 只有少量沒使用的物件時，這可能會非常的低效，如 slab 頁面有 10 個物件，只有一個物件還在使用，剩下 9 個都已經釋放，該頁面還沒辦法被釋放，導致 slab cache 有大量內部碎裂化的問題\n另外是回收侷限性，如果記憶體不足或是壓力大，slab 會回收 cache page，但是回收策略可能不僅線單一節點或是具體 slab cache page。通常回收會是全域的，這可能導致回收物件分配在多個 slab page, 而不是集中回收\n如果回收過多 slab cache 的物件，是否會破壞 cache 的 locality，影響未來的效能\n上面問題解決方向，大部分會往更細粒度的回收策略進行，希望有一些 API 讓我們可以區分一些情況，針對情況去優化等等\n","date":"2025-10-02T10:52:22+08:00","permalink":"https://wuta0209.github.io/p/linux_numa_system/","title":"Linux_NUMA_System"},{"content":"NV Jetson Debugging, Update BootLoader 問題描述 發現只要開機之前插入 webcam 就會無法開機，沒有畫面，推測是在 UEFI 或是 Second Bootloader 的時候發生錯誤，後面 Thrid Bootloader 到 Linux 都沒有被成功載入。\n通過聽風扇轉速判斷在第一次失敗之後有重新開機過一次，在開機的時候風扇轉速較大，接著轉速維持在一定速率。\n由於黑畫面，沒有任何訊息，這時候沒有 printf, gdb 那一些資源可以使用，直覺想到也許可以通過 GPIO, UART，但需要修改 UEFI 讓他的 TX 送出啟動階段的訊息，這時候先查詢 Jetson 使用 GPIO 的方法，發現除了側面 40 pin 的 GPIO 之外還有後面的 micro-USB 可以使用，且 UEFI 會通過這個界面送出訊息，只要接收端設置 baudrate 為 115200 就可以收到訊息。\n通過 micro-usb 得到 Debug 訊息 將 micro-usb 與 host 相互連接之後會出現 ttyACM0 ~ ttyACM3 設備，選取 ttyAMA0 通過 minicom 成功讀取到啟動階段訊息，方便處理我們直接將訊息寫檔之後讀出\n1 2 ls /dev/ttyACM* sudo minicom -D /dev/ttyACM -b 115200 -8 -o -C \u0026lt;output_file_name\u0026gt; 訊息如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 CPU switching to normal world boot Jetson UEFI firmware (version 36.4.4-gcid-41062509 built on 2025-06-16T15:25:51+00:00) I/TC: Reserved shared memory is disabled I/TC: Dynamic shared memory is enabled I/TC: Normal World virtualization support is disabled I/TC: Asynchronous notifications are disabled I/TC: WARNING: Test UEFI variable auth key is being used ! I/TC: WARNING: UEFI variable protection is not fully enabled ! [ 6.318531] Camera-FW on t234-rce-safe started TCU early console enabled. [ 6.380337] Camera-FW on t234-rce-safe ready SHA1=e2238c99 (crt 1.360 ms, total boot 63.229 ms) 3h ASSERT [XhciDxe] /out/nvidia/bootloader/uefi/Jetson_RELEASE/edk2/MdeModulePkg/Bus/Pci/XhciDxe/XhciSched.c(3154): Interval \u0026gt;= 1 \u0026amp;\u0026amp; Interval \u0026lt;= 16 Resetting the system in 5 seconds. Shutdown state requested 1 Rebooting system ... [0000.062] I\u0026gt; MB1 (version: 1.4.0.4-t234-54845784-e89ea9bc) [0000.067] I\u0026gt; t234-A01-1-Silicon (0x12347) Prod [0000.072] I\u0026gt; Boot-mode : Coldboot [0000.075] I\u0026gt; Entry timestamp: 0x00000000 [0000.078] I\u0026gt; last_boot_error: 0x0 [0000.082] I\u0026gt; BR-BCT: preprod_dev_sign: 0 [0000.085] I\u0026gt; rst_source: 0xb, rst_level: 0x1 [0000.089] I\u0026gt; Task: SE error check [0000.093] I\u0026gt; Task: Bootchain select WAR set [0000.097] I\u0026gt; Task: Enable SLCG [0000.100] I\u0026gt; Task: CRC check [0000.102] I\u0026gt; Task: Initialize MB2 params [0000.107] I\u0026gt; MB2-params @ 0x40060000 [0000.110] I\u0026gt; Task: Crypto init [0000.113] I\u0026gt; Task: Perform MB1 KAT tests [0000.117] I\u0026gt; Tas [0000.124] I\u0026gt; Task: MSS Bandwidth limiter settings for iGPU clients [0000.130] I\u0026gt; Task: Enabling and initialization of Bandwidth limiter [0000.136] I\u0026gt; No request to configure MBWT settings for any PC! [0000.142] I\u0026gt; Task: Secure debug controls [0000.146] I\u0026gt; Task: strap war set [0000.149] I\u0026gt; Task: Initialize SOC Therm [0000.153] I\u0026gt; Task: Program NV master stream id [0000.157] I\u0026gt; Task: Verify boot mode [0000.163] I\u0026gt; Task: Alias fuses [not supported. [0000.173] I\u0026gt; Task: Print SKU type [0000.177] I\u0026gt; FUSE_OPT_CCPLEX_CLUSTER_DISABLE = 0x00000000 [0000.182] I\u0026gt; FUSE_OPT_GPC_DISABLE = 0x00000000 [0000.186] I\u0026gt; FUSE_OPT_TPC_DISABLE = 0x00000000 [0000.190] I\u0026gt; FUSE_OPT_DLA_DISABLE = 0x00000000 [0000.195] I\u0026gt; FUSE_OPT_PVA_DISABLE = 0x00000000 [0000.199] I\u0026gt; FUSE_OPT_NVENC_DISABLE = 0x00000000 [0000.203] I\u0026gt; FUSE_OPT_NVDEC_DISABLE = 0x00000000 [0000.208] I\u0026gt; FUSE_OPT_FSI_DISABLE = 0x00000000 [0000.212] I\u0026gt; FUSE_OPT_EMC_DISABLE = 0x00000000 [0000.216] I\u0026gt; FUSE_BOOTROM_PATCH_VERSION = 0x7 [0000.221] I\u0026gt; FUSE_PSCROM_PATCH_VERSION = 0x7 [0000.225] I\u0026gt; FUSE_OPT_ADC_CAL_FUSE_REV = 0x2 [0000.229] I\u0026gt; FUSE_SKU_INFO_0 = 0xd0 [0000.232] I\u0026gt; FUSE_OPT_SAMPLE_TYPE_0 = 0x3 PS [0000.236] I\u0026gt; FUSE_PACKAGE_INFO_0 = 0x2 [0000.240] I\u0026gt; SKU: Prod [0000.242] I\u0026gt; Task: Boost clocks [0000.245] I\u0026gt; Initializing NAFLL for BPMP_CPU_NIC. [0000.250] I\u0026gt; BPMP NAFLL: fll_lock = 1, dvco_min_reached = 0 [0000.256] 42, divisor = 0 [0000.264] I\u0026gt; Initializing PLLC2 for AXI_CBB. [0000.268] I\u0026gt; AXI_CBB : src = 35, divisor = 0 [0000.272] I\u0026gt; Task: Voltage monitor [0000.275] I\u0026gt; VMON: Vmon re-calibration andHSIO UPHY init done [0000.289] W\u0026gt; Skipping GBE UPHY config [0000.292] I\u0026gt; Task: Boot device init [0000.295] I\u0026gt; Boot_device: QSPI_FLASH instance: 0 [0000.300] I\u0026gt; Qspi clock source : pllc_out0 [0000.304] I\u0026gt; QSPI Flash: Macronix 64MB [0000.308] I\u0026gt; QSPI-0315] I\u0026gt; Task: Load membct [0000.318] I\u0026gt; RAM_CODE 0x4000431 [0000.321] I\u0026gt; Loading MEMBCT [0000.324] I\u0026gt; Slot: 0 [0000.326] I\u0026gt; Binary[0] block-3840 (partition size: 0x40000) [0000.331] I\u0026gt; B92 [0000.338] I\u0026gt; Size of crypto header is 8192 [0000.342] I\u0026gt; strt_pg_num(3840) num_of_pgs(16) read_buf(0x40050000) [0000.348] I\u0026gt; BCH of MEM-BCT-0 read from storage [0000.353] I\u0026gt; BCH address is : 0x40050000 [0000.357] I\u0026gt; MEM-BCT-0 header integrity checEM0 [0000.367] I\u0026gt; component binary type is 0 [0000.370] I\u0026gt; strI\u0026gt; MEM-BCT-0 binary is read from storage [0000.382] I\u0026gt; MEM-BCT-0 binary integrity check is success [0000.387] I\u0026gt; Binary MEM-BCT-0 loaded successfully at 0x40040000 (0xe580) [0000.394] I\u0026gt; RAM_CODE 0x4000431 [0000.399] I\u0026gt; RAM_CODE 0x4000431 [0000.403] Iams override [0000.411] I\u0026gt; Task: Save mem-bct info [0000.414] I\u0026gt; Task: Carveout allocate [0000.418] I\u0026gt; RCM blob carveout will not be allocated [0000.423] I\u0026gt; Update CCPLEX IST carveout from MB1-BCT [0000.427] I\u0026gt; ECC region[0]: Start:0x0, End:0x0 [0000.432] I\u0026gt; ECC region[1]: Start:0x0, End:0x0 [0000.436] I\u0026gt; ECC region[2]: Start:0x0, End:0x0 [0000.440] I\u0026gt; ECC region[3]: Start:0x0, End:0x0 [0000.445] I\u0026gt; ECC region[4]: Start:0x0, End:0x0 [0000.449] I\u0026gt; Non-ECC region[0]: Start:0x80000000, End:0x10800000ECC region[3]: Start:0x0, End:0x0 [0000.469] I\u0026gt; Non-ECC region[d(CO:31) base:0x1040000000 size:0x8000000 align: 0x8000000 [0000.494] I\u0026gt; allocated(CO:43) base:0x103c000000 size:0x4000000 align: 0x200000 [0000.501] I\u0026gt; allocated(CO:39) base:0x1039e00000 size:0x2200000 align: 0x10000 [0000.508] I\u0026gt; allocated(CO:20) base:0x1036000000 size:0x2000000 align: 0x2000000 [0000.515] I\u0026gt; allocated(CO:24) base:0x1034000000 size:0x2000000 align: 0x2000000 [0000.523] I\u0026gt; allocated(CO:28) base:0x1032000000 size:0x2000000 align: 0x2000000 [0000.530] I\u0026gt; allocated(CO:29) base:0x1030000000 size:0x2000000 align: 0x2000000 [0000.537] I\u0026gt; allocated(CO:22) base:0x1048000000 size:0x1000000 align: 0x1000000 [0000.544] I\u0026gt; allocated(CO:35) base:0x1038e00000 size:0x1000000 align: 0x100000 [0000.551] I\u0026gt; allocated(CO:41) base:0x102f000000 size:0x1000000 align: 0x100000 [0000.558] I\u0026gt; allocated(CO:02) base:0x1049000000 size:0x800000 align: 0x800000 [0000.565] I\u0026gt; allocated(CO:03) base:0x1038000000 size:0x800000 align: 0x800000 [0000.572] I\u0026gt; allocated(CO:06) base:0x102e800000 size:0x800000 align: 0x800000 [0000.579] I\u0026gt; allocated(CO:56) base:0x102e000000 size:0x800000 align: 0x200000 [0000.586] I\u0026gt; allocated(CO:07) base:0x1038800000 size:0x400000 align: 0x400000 [0000.594] I\u0026gt; allocated(CO:33) base:0x102dc00000 size:0x400000 align: 0x200000 [0000.601] I\u0026gt; allocated(CO:19) base:0x102d980000 size:0x280000 align: 0x10000 [0000.607] I\u0026gt; allocated(CO:23) base:0x1038c00000 size:0x200000 align: 0x200000 [0000.615] I\u0026gt; allocated(CO:01) base:0x102d800000 size:0x100000 align: 0x100000 [0000.622] I\u0026gt; allocated(CO:05) base:0x102d700000 size:0x100000 align: 0x100000 [0000.629] I\u0026gt; allocated(CO:08) base:0x102d600000 size:0x100000 align: 0x100000 [0000.636] I\u0026gt; allocated(CO:09) base:0x102d500000 size:0x100000 align: 0x100000 [0000.643] I\u0026gt; allocated(CO:12) base:0x102d400000 size:0x100000 align: 0x100000 [0000.650] I\u0026gt; allocated(CO:15) base:0x102d300000 size:0x100000 align: 0x100000 [0000.657] I\u0026gt; allocated(CO:17) base:0x102d200000 size:0x100000 align: 0x100000 [0000.664] I\u0026gt; allocated(CO:27) base:0x102d100000 size:0x100000 align: 0x100000 [0000.671] I\u0026gt; allocated(CO:42) base:0x102d000000 size:0x100000 align: 0x100000 [0000.678] I\u0026gt; allocated(CO:54) base:0x102d900000 size:0x80000 align: 0x80000 [0000.685] I\u0026gt; allocated(CO:34) base:0x102cff0000 size:0x10000 align: 0x10000 [0000.692] I\u0026gt; allocated(CO:72) base:0x102cdf0000 size:0x200000 align: 0x10000 [0000.698] I\u0026gt; allocated(CO:47) base:0x102c800000 size:0x400000 align: 0x200000 [0000.706] I\u0026gt; allocated(CO:50) base:0x102c600000 size:0x200000 align: 0x100000 [0000.713] I\u0026gt; allocated(CO:52) base:0x102cdc0000 size:0x30000 align: 0x10000 [0000.719] I\u0026gt; allocated(CO:48) base:0x102cda0000 size:0x20000 align: 0x10000 [0000.726] I\u0026gt; allocated(CO:69) base:0x102cd80000 size:0x20000 align: 0x10000 [0000.733] I\u0026gt; allocated(CO:49) base:0x102cd70000 size:0x10000 align: 0x10000 [0000.740] I\u0026gt; NSDRAM base: 0x80000000, end: 0x102cdf0000, size: 0xfacdf0000 [0000.747] I\u0026gt; Task: Thermal check [0000.750] I\u0026gt; Using min_chip_limit as min_tmon_limit [0000.755] I\u0026gt; Using max_chip_limit as max_tmon_limit [0000.759] I\u0026gt; BCT max_tmon_limit = 105 [0000.763] I\u0026gt; BCT min_tmon_limit = -28 [0000.766] I\u0026gt; BCT max_tmon_limit = 105 [0000.770] I\u0026gt; BCT min_tmon_limit = -28 [0000.773] I\u0026gt; SKU specific max_chip_limit = 105 [0000.777] I\u0026gt; SKU specific min_chip_limit = -28 [0000.782] I\u0026gt; BCT max_chip_limit = 105 [0000.785] I\u0026gt; BCT min_chip_limit = -28 [0000.789] I\u0026gt; enable_soctherm_polling = 0 [0000.793] I\u0026gt; max temp read = 37 [0000.795] I\u0026gt; min temp read = 36 [0000.798] I\u0026gt; Enabling thermtrip [0000.801] I\u0026gt; Task: Update FSI SCR with thermal fuse data [0000.807] I\u0026gt; Task: Enable WDT 5th expiry [0000.810] I\u0026gt; Task: I2C register [0000.813] I\u0026gt; Task: Set I2C bus freq [0000.817] I\u0026gt; Task: Reset FSI [0000.819] I\u0026gt; Task: Pinmux init [0000.823] I\u0026gt; Task: Prod config init [0000.826] I\u0026gt; Task: Pad voltage init [0000.829] I\u0026gt; Task: Prod init [0000.832] I\u0026gt; Task: Program rst req config reg [0000.836] I\u0026gt; Task: Common rail init [0000.841] W\u0026gt; DEVICE_PROD: module = 13, instance = 4 not found in device prod. [0000.851] I\u0026gt; DONE: Thermal config [0000.856] I\u0026gt; DONE: SOC rail config [0000.859] W\u0026gt; PMIC_CONFIG: Rail: MEMIO rail config not found in MB1 BCT. [0000.866] I\u0026gt; DONE: MEMIO rail config [0000.869] I\u0026gt; DONE: GPU rail info [0000.873] I\u0026gt; DONE: CV rail info [0000.876] I\u0026gt; Task: Mem clock src [0000.879] I\u0026gt; Task: Misc. board config [0000.883] I\u0026gt; PMIC_CONFIG: Platform config not found in MB1 BCT. [0000.889] I\u0026gt; Task: SDRAM init [0000.892] I\u0026gt; MemoryType: 4 MemBctRevision: 8 [0000.899] I\u0026gt; MSS CAR: PLLM/HUB programming for MemoryType: 4 and MemBctRevision: 8 [0000.906] I\u0026gt; MSS CAR: Init PLLM [0000.909] I\u0026gt; MSS CAR: Init PLLHUB [0000.914] I\u0026gt; Encryption: MTS: en, TX: en, VPR: en, GSC: en [0000.926] I\u0026gt; SDRAM initialized! [0000.929] I\u0026gt; SDRAM Size in Total 0x1000000000 [0000.933] I\u0026gt; Task: Dram Ecc scrub [0000.936] I\u0026gt; Task: DRAM alias check [0000.952] I\u0026gt; Task: Program NSDRAM carveout [0000.956] I\u0026gt; NSDRAM carveout encryption is enabled [0000.961] I\u0026gt; Program NSDRAM ck: Enable clock-mon [0000.982] I\u0026gt; FMON: Fmon re-programming done [0000.986] I\u0026gt; Task: Mapper init [0000.989] I\u0026gt; Task: SC7 Context Init [0000.993] I\u0026gt; Task: CCPLEX IST init [0000.996] I\u0026gt; Task: CPU WP0 [0001.000] I\u0026gt; Loading MCE [0001.002] I\u0026gt; Slot: 0 [0001.004] I\u0026gt; Binary[8] block-22784 (partition size: 0x80000) [00 is 8192 [0001.016] I\u0026gt; Size of crypto header is 8192 [0001.020] I\u0026gt; strt_pg_num(22784) num_of_pgs(16) read_buf(0x4003e000) [0001.027] I\u0026gt; BCH of MCE read from storage [0001.031] I\u0026gt; BCH addresuccess [0001.039] I\u0026gt; Binary magic in BCH component 0 is MTSM [0001.044] I\u0026gt; component binary type is 8 [0001.048] I\u0026gt; Size of crypto header is 8192 [0001.051] I\u0026gt; strt_pg_num(22800) num_of_pgs(350) read_buf(0x40000000) [0001.060] I\u0026gt; MCE binary is read from storage [0001.064] I\u0026gt; MCE binary integrity check is success [0001.069] I\u0026gt; Binary MCE loaded successfully at 0x40000000 (0x2baf0) [0001.075] I\u0026gt; Size of crypto header is 8192 [0001.086] I\u0026gt; Size of crypto header is 8192 [0001.090] I\u0026gt; Sending WP0 mailbox command to PSC [0001.099] I\u0026gt; Task: XUSB Powergate [0001.102] I\u0026gt; Skipping powergate XUSB. [0001.106] I\u0026gt; Task: MB1 fixed firewalls [0001.112] W\u0026gt; Firewall readback mismatch [0001.117] I\u0026gt; Task: Load bpmp-fw [0001.120] I\u0026gt; Slot: 0 [0001.122] I\u0026gt; Binary[15] block-9984 (partition size: 0x180000) [0001.128] I\u0026gt; Binary name: BPMP_FW [0001.131] I\u0026gt; Size of crypto header is 8192 [0001.135] I\u0026gt; Size of crypto header is 8192 [0001.139] I\u0026gt; strt_pg_num(9984) num_of_pgs(16) read_buf(0x807fe000) [0001.145] I\u0026gt; BCH of BPMP_FW read from storage [0001.149] I\u0026gt; BCH address is : 0x807fe000 [0001.153] I\u0026gt; BPMP_FW header integrity check is success [0001.158] I\u0026gt; Binary magic in BCH component 0 is BPMF [0001.163] I\u0026gt; component binary type is 15 [0001.166] I\u0026gt; Size of crypto header is 8192 [0001.170] I\u0026gt; strt_pg_num(10000) num_of_pgs(1990) read_buf(0x80000000) [0001.188] I\u0026gt; BPMP_FW binary is read from storage [0001.194] I\u0026gt; BPMP_FW binary integrity check is success [0001.199] I\u0026gt; Binary BPMP_FW loaded successfully at 0x80000000 (0xf8bc0) [0001.206] I\u0026gt; Slot: 0 [0001.208] I\u0026gt; Binary[16] block-13056 (partition size: 0x400000) [0001.213] I\u0026gt; Binary name: BPMP_FW_DTB [0001.217] I\u0026gt; Size of crypto header is 8192 [0001.221] I\u0026gt; Size of crypto header is 8192 [0001.225] I\u0026gt; strt_pg_num(13056) num_of_pgs(16) read_buf(0x807fc000) [0001.231] I\u0026gt; BCH of BPMP_FW_DTB read from storage [0001.236] I\u0026gt; BCH address is : 0x807fc000 [0001.239] I\u0026gt; BPMP_FW_DTB header integrity check is success [0001.245] I\u0026gt; Binary magic in BCH component 0 is BPMD [0001.250] I\u0026gt; component binary type is 16 [0001.253] I\u0026gt; Size of crypto header is 8192 [0001.257] I\u0026gt; strt_pg_num(13072) num_of_pgs(502) read_buf(0x807bd3f0) [0001.266] I\u0026gt; BPMP_FW_DTB binary is read from storage [0001.272] I\u0026gt; BPMP_FW_DTB binary integrity check is success [0001.277] I\u0026gt; Binary BPMP_FW_DTB loaded successfully at 0x807bd3f0 (0x3eb40) [0001.284] I\u0026gt; Task: BPMP fw ast config [0001.287] I\u0026gt; Task: Load psc-fw [0001.290] I\u0026gt; Slot: 0 [0001.292] I\u0026gt; Binary[17] block-21248 (partition size: 0xc0000) [0001.298] I\u0026gt; Binary name: PSC_FW [0001.301] I\u0026gt; Size of crypto header is 8192 [0001.305] I\u0026gt; Size of crypto header is 8192 [0001.309] I\u0026gt; strt_pg_num(21248) num_of_pgs(16) read_buf(0x80ffe000) [0001.315] I\u0026gt; BCH of PSC_FW read from storage [0001.319] I\u0026gt; BCH address is : 0x80ffe000 [0001.323] I\u0026gt; PSC_FW header integrity check is success [0001.328] I\u0026gt; Binary magic in BCH component 0 is PFWP [0001.333] I\u0026gt; component binary type is 17 [0001.337] I\u0026gt; Size of crypto header is 8192 [0001.340] I\u0026gt; strt_pg_num(21264) num_of_pgs(591) read_buf(0x80fb4200) [0001.350] I\u0026gt; PSC_FW binary is read from storage [0001.355] I\u0026gt; PSC_FW binary integrity check is success [0001.360] I\u0026gt; Binary PSC_FW loaded successfully at 0x80fb4200 (0x49df0) [0001.366] I\u0026gt; Task: Load nvdec-fw [0001.369] I\u0026gt; Slot: 0 [0001.371] I\u0026gt; Binary[7] block-6400 (partition size: 0x100000) [0001.377] I\u0026gt; Binary name: NVDEC [0001.380] I\u0026gt; Size of crypto header is 8192 [0001.384] I\u0026gt; Size of crypto header is 8192 [0001.388] I\u0026gt; strt_pg_num(6400) num_of_pgs(16) read_buf(0x800fe000) [0001.394] I\u0026gt; BCH of NVDEC read from storage [0001.398] I\u0026gt; BCH address is : 0x800fe000 [0001.402] I\u0026gt; NVDEC header integrity check is success [0001.407] I\u0026gt; Binary magic in BCH component 0 is NDEC [0001.411] I\u0026gt; component binary type is 7 [0001.415] I\u0026gt; Size of crypto header is 8192 [0001.419] I\u0026gt; strt_pg_num(6416) num_of_pgs(560) read_buf(0x80000000) [0001.428] I\u0026gt; NVDEC binary is read from storage [0001.433] I\u0026gt; NVDEC binary integrity check is success [0001.438] I\u0026gt; Binary NVDEC loaded successfully at 0x80000000 (0x46000) [0001.444] I\u0026gt; Size of crypto header is 8192 [0001.456] I\u0026gt; Task: Load tsec-fw [0001.459] I\u0026gt; TSEC-FW load support not enabled [0001.463] I\u0026gt; Task: GPIO interrupt map [0001.467] I\u0026gt; Task: SC7 context save [0001.470] I\u0026gt; Slot: 0 [0001.472] I\u0026gt; Binary[27] block-0 (partition size: 0x100000) [0001.477] I\u0026gt; Binary name: BR_BCT [0001.480] I\u0026gt; Size of crypto header is 8192 [0001.484] I\u0026gt; Size of crypto header is 8192 [0001.488] I\u0026gt; Size of crypto header is 8192 [000001.498] I\u0026gt; BR_BCT binary is read from storage [0001.503] I\u0026gt; BR_BCT binary integrity check is success [0001.507] I\u0026gt; Binary BRlot: 0 [0001.516] I\u0026gt; Binary[13] block-23808 (partition size: 0x30000) [0001.521] I\u0026gt; Binary name: SC7-FW [0001.524] I\u0026gt; Size of crypto header is 8192 [0001.528] I\u0026gt; Size of crypto header is 8192 [0001.532] I\u0026gt; Size of crypto header is 8192 [0001.536] I\u0026gt; Size of crypto header is 8192 [0001.540] I\u0026gt; strt_pg_num(23808) num_of_pgs(16) read_buf(0xa0002000) [0001.546] I\u0026gt; BCH of SC7-FW read from storage [0001.551] I\u0026gt; BCH address is : 0xa0002000 [0001.554] I\u0026gt; SC7-FW header integrity check is success [0001.559] I\u0026gt; Binary magic in BCH component 0 is WB0B [0001.564] I\u0026gt; component binary type is 13 [0001.568] I\u0026gt; Size of crypto header is 8192 [0001.572] I\u0026gt; strt_pg_num(23824) num_of_pgs(349) read_buf(0xa0004000) [0001.580] I\u0026gt; SC7-FW binary is read from storage [0001.585] I\u0026gt; SC7-FW binary integrity check is success [0001.590] I\u0026gt; Binary SC7-FW loaded successfully at 0xa0004000 (0x2ba00) [0001.596] I\u0026gt; Slot: 0 [0001.598] I\u0026gt; Binary[22] block-24192 (partition size: 0x30000) [0001.604] I\u0026gt; Binary name: PSC_RF [0001.607] I\u0026gt; Size of crypto header is 8192 [0001.611] I\u0026gt; Size of crypto header is 8192 [0001.615] I\u0026gt; Size of crypto header is 8192 [0001.618] I\u0026gt; Size of crypto header is 8192 [0001.622] I\u0026gt; strt_pg_num(24192) num_of_pgs(16) read_buf(0xa002fa00) [0001.629] I\u0026gt; BCH of PSC_RF read from storage [0001.633] I\u0026gt; BCH address is : 0xa002fa00 [0001.637] I\u0026gt; PSC_RF header integrity check is success [0001.641] I\u0026gt; Binary magic in BCH component 0 is PSCR [00(224) read_buf(0xa0031a00) [0001.662] I\u0026gt; PSC_RF binary is read a00 (0x1be60) [0001.680] I\u0026gt; Task: Save WP0 payload to SC7 ctx [0001.685] I\u0026gt; Task: Load MB2rf binary to SC7 ctx [0001.689] I\u0026gt; Slot: 0 [0001.691] I\u0026gt; Binary[14] block-24576 (partition size: 0x20000) [0001.697] I\u0026gt; Binary name: MB2_RF [0001.700] I\u0026gt; Size of crypto header is 8192 [0001.704] I\u0026gt; Size of crypto header is 8192 [0001.708] I\u0026gt; Size of crypto header is 8192 [0001.712] I\u0026gt; Size of crypto header is 8192 [0001.715] I\u0026gt; strt_pg_num(24576) num_of_pgs(16) read_buf(0xa00d5d10) [0001.722] I\u0026gt; BCH of MB2_RF read from storage [0001.726] I\u0026gt; BCH address is : 0xa00d5d10 [0001.730] I\u0026gt; MB2_RF header integrity check is success [0001.735] I\u0026gt; Binary magic in BCH component 0 is MB2R [0001.739] I\u0026gt; component binary type is 14 [0001.743] I\u0026gt; Size of crypto header is 8192 [0001.747] I\u0026gt; strt_pg_num(24592) num_of_pgs(224) read_buf(0xa00d7d10) [0001.755] I\u0026gt; MB2_RF binary is read from storage [0001.759] I\u0026gt; MB2_RF binary integrity check is success [0001.764] I\u0026gt; Binary MB2_RF loaded successfully at 0xa00d7d10 (0x1bf60) [0001.771] I\u0026gt; Task: Save fuse alias data to SC7 ctx [0001.775] I\u0026gt; Task: Save PMIC data to SC7 ctx [0001.779] I\u0026gt; Task: Save Pinmux data to SC7 ctx [0001.784] I\u0026gt; Task: Save Pad Voltage data to SC7 ctx [0001.788] I\u0026gt; Task: Save controller prod data to SC7 ctx [0001.793] I\u0026gt; Task: Save prod cfg data to SC7 ctx [0001.798] I\u0026gt; Task: Save I2C bus freq data to SC7 ctx [0001.803] I\u0026gt; Task: Save SOCTherm data to SC7 ctx [0001.807] I\u0026gt; Task: Save FMON data to SC7 ctx [0001.811] I\u0026gt; Task: Save VMON data to SC7 ctx [0001.815] I\u0026gt; Task: Save TZDRAM data to SC7 ctx [0001.820] I\u0026gt; Task: Save GPIO int data to SC7 ctx [0001.824] I\u0026gt; Task: Save clock data to SC7 ctx [0001.828] I\u0026gt; Task: Save debug data to SC7 ctx [0001.832] I\u0026gt; Task: Save MBWT data to SC7 ctx [0001.840] I\u0026gt; SC7 context save done [0001.844] I\u0026gt; Task: Load MB2/Applet/FSKP [0001.847] I\u0026gt; Loading MB2 [0001.850] I\u0026gt; Slot: 0 [0001.852] I\u0026gt; Binary[6] block-8448 (partition size: 0x80000) [0001.857] I\u0026gt; Binary name: MB2 [0001.860] I\u0026gt; Size of crypto header is 8192 [0001.864] I\u0026gt; Size of crypto header is 8192 [0001.868] I\u0026gt; strt_pg_num(8448) num_of_pgs(16) read_buf(0x8007e000) [0001.874] I\u0026gt; BCH of MB2 read from storage [0001.878] I\u0026gt; BCH address is : 0x8007e000 [0001.882] I\u0026gt; MB2 header integrity check is success [0001.886] I\u0026gt; Binary magic in BCH component 0 is MB2B [0001.891] I\u0026gt; component binary type is 6 [0001.895] I\u0026gt; Size of crypto header is 8192 [0001.899] I\u0026gt; strt_pg_num(8464) num_of_pgs(846) read_buf(0x80000000) [0001.910] I\u0026gt; MB2 binary is read from storage [0001.915] I\u0026gt; MB2 binary integrity check is success [0001.919] I\u0026gt; Binary MB2 loaded successfully at 0x80000000 (0x69a70) [0001.926] I\u0026gt; Task: Map CCPLEX SHARED carveout [0001.930] I\u0026gt; Task: Prepare MB2 params [0001.934] I\u0026gt; Task: Dram ecc test [0001.937] I\u0026gt; Task: Misc NV security settings [0001.941] I\u0026gt; NVDEC sticky bits programming done [0001.945] I\u0026gt; Successfully powergated NVDEC [0001.949] I\u0026gt; Task: Disable/Reload WDT [0001.953] I\u0026gt; Task: Program misc carveouts [0001.957] I\u0026gt; Program IPC carveouts [0001.960] I\u0026gt; Task: Disable SCPM/POD reset [0001.964] I\u0026gt; SLCG Global override status := 0x0 [0001.968] I\u0026gt; MB1: MSS reconfig completed I\u0026gt; MB2 (version: 0.0.0.0-t234-54845784-22833a33) I\u0026gt; t234-A01-1-Silicon (0x12347) I\u0026gt; Boot-mode : Coldboot I\u0026gt; Emulation: I\u0026gt; Entry timestamp: 0x001e7647 I\u0026gt; Regular heap: [base:0x40040000, size:0x10000] I\u0026gt; DMA heap: [base:0x102e000000, size:0x800000] I\u0026gt; Task: SE error check I\u0026gt; Task: Crypto init I\u0026gt; Task: MB2 Params integrity check I\u0026gt; Task: Enable CCPLEX WDT 5th expiry I\u0026gt; Task: ARI update carveout TZDRAM I\u0026gt; Task: Configure OEM set LA/PTSA values I\u0026gt; Task: Check MC errors I\u0026gt; Task: SMMU external bypass disable I\u0026gt; Task: Enable hot-plug capability I\u0026gt; Task: TZDRAM heap init I\u0026gt; Task: PSC mailbox init I\u0026gt; Task: Enable clock for external modules I\u0026gt; Task: Measured Boot init I\u0026gt; Task: fTPM silicon identity init I\u0026gt; fTPM is not enabled. I\u0026gt; Task: OEM SC7 context save init I\u0026gt; Task: I2C register I\u0026gt; Task: Map CCPLEX_INTERWORLD_SHMEM carveout I\u0026gt; Task: Program CBB PCIE AMAP regions I\u0026gt; Task: Boot device init I\u0026gt; Boot_device: QSPI_FLASH instance: 0 I\u0026gt; QSPI-0l initialized successfully I\u0026gt; Secondary storage device: QSPI_FLASH instance: 0 I\u0026gt; Secondary storage device: SDMMC_USER instance: 3 I\u0026gt; sdmmc HS400 mode enabled I\u0026gt; Task: Partition Manager Init I\u0026gt; strt_pg_num(1) num_of_pgs(1) read_buf(0x102e001000) I\u0026gt; strt_pgm(131039) num_of_pgs(32) read_buf(0x102e001200) I\u0026gt; Found 60 partitions in QSPI_FLASH (instance 0) W\u0026gt; Cannot find any partition table for 00000003 W\u0026gt; PARTITION_MANAGER: Failed to publish partition. I\u0026gt; Found 15 partitions in SDMMC_USER (instance 3) I\u0026gt; Task: Pass DRAM ECC PRL Flag to FSI I\u0026gt; Task: Load and authenticate registered FWs I\u0026gt; Task: Load AUXP FWs I\u0026gt; Successfully regisCE FW load task with MB2 loader I\u0026gt; Successfully register DCE FW load task with MB2 loader I\u0026gt; Unpowergating APE I\u0026gt; Unpowergate done I\u0026gt; Successfully register APE FW load task with MB2 loader I\u0026gt; Skipping FSI FW load I\u0026gt; Successfully register XUSB FW load task with MB2 loader I\u0026gt; Successfully register PVA FW load task with MB2 loader I\u0026gt; Partition name: A_spe-fw I\u0026gt; Size of partition: 589824 I\u0026gt; Binary@ device:3/0 block-55040 (partition size: 0x90000), name: A_spe-fw I\u0026gt; strt_pg_num(55040) num_of_pgs(16) read_buf(0x40067a30) I\u0026gt; strt_pg_num(55056) num_of_pgs(512) read_buf(0x102d600000) I\u0026gt; Partition name: A_rce-fw I\u0026gt; Size of partition: 1048576 I\u0026gt; Binary@ device:3/0 block-56192 (partition size) read_buf(0x40067a30) I\u0026gt; strt_pg_num(56208) num_of_pgs(880) reinary spe loaded successfully at 0x102d600000 I\u0026gt; Partition name: A_dce-fw I\u0026gt; Size of partition: 5242880 I\u0026gt; Binary@ device:3/0_pg_num(44800) num_of_pgs(16) read_buf(0x40067a30) I\u0026gt; rce: Authentication Finalize Done I\u0026gt; Binary rce loaded successfully at 0x102d200000 I\u0026gt; Successfully register RCE FW context save task wtrt_pg_num(44816) num_of_pgs(1) read_buf(0x102e1403d8) I\u0026gt; strt_a-blob integrity check is success. I\u0026gt; strt_pg_num(44824) num_of_pgs(512) read_buf(0x102e0003c0) I\u0026gt; strt_pg_num(45336) num_of_p 0x1036000000 I\u0026gt; version 1 Bin 1 BCheckSum 0 content_size 0 Conerved11 0 I\u0026gt; strt_pg_num(45848) num_of_pgs(512) read_buf(0x102e0803c0) I\u0026gt; dce : decompressed to 12067600 bytes I\u0026gt; dce: plain binary integrity check is success I\u0026gt; Partition name: A_adsp-fw I\u0026gt; Size of partition: 2097152 I\u0026gt; Binary@ device:strt_pg_num(58240) num_of_pgs(16) read_buf(0x40067a30) I\u0026gt; strt_ 0x1036000000 I\u0026gt; Partition name: A_xusb-fw I\u0026gt; Size of partition: 262144 I\u0026gt; Binary@ device:3/0 block-9472 (partition size: 0x40000), name: A_xusb-fw I\u0026gt; strt_pg_num(9472) num_of_pgs(16) read_buf(0x40067a30) I\u0026gt; strt_pg_num(9488) num_of_pgs(312) read_buf(0x102d700000) I\u0026gt; ape: Authentication Finalize Done I\u0026gt; Binary ape loaded successfully at 0x1038800000 I\u0026gt; Successfully register APE FW context save task with MB2 loader I\u0026gt; Partition name: A_pva-fw I\u0026gt; Size of partition: 262144 I\u0026gt; Binary@ device:3/0 block-62336 (partition size: 0x40000), name: A_pva-fw I\u0026gt; strt_pg_num(62336) num_of_pgs(16) read_buf(0x40067a30) I\u0026gt; xusb: Authentication Finalize Done I\u0026gt; Binary xusb loaded successfully at 0x102d700000 I\u0026gt; Successfully register XUSB FW context save task with MB2 loader I\u0026gt; pva-fw : oem authentication of header done I\u0026gt; strt_pg_num(62352) num_of_pgs(1) read_buf(0x102e1403d8) I\u0026gt; strt_pg_num(62352) num_of_pgs(8) read_buf(0x102e1403d8) I\u0026gt; pva-fw : meta-blob integrity check is success. I\u0026gt; strt_pg_num(62360) num_of_pgs(512) read_buf(0x102e0003c0) I\u0026gt; pva-fw : will be decompressed at 0x102d980000 I\u0026gt; version 1 Bin 1 BCheckSum 0 content_size 0 Content ChkSum 1 reserved_00 0 I\u0026gt; Reserved10 0 BlockMaxSize 5 Reserved11 0 I\u0026gt; pva-fw : decompressed to 2156512 bytes I\u0026gt; pva-fw: plain binary integrity check is success I\u0026gt; pva-fw: Authentication Finalize Done I\u0026gt; Binary pva-fw loaded successfully at 0x102d980000 I\u0026gt; Successfully register PVA FW context save task with MB2 loader I\u0026gt; Task: Check MC errors I\u0026gt; Task: Carveout setup I\u0026gt; Program remaining OEM carveouts I\u0026gt; Task: Enable FSITHERM I\u0026gt; Task: Enable FSI VMON I\u0026gt; FSI VMON: FSI Vmon re-calibration and fine tuning done I\u0026gt; Task: Validate FSI Therm readings I\u0026gt; Task: Restore XUSB sec I\u0026gt; Task: Enable FSI SE clock I\u0026gt; Enable FSI-SE clock... I\u0026gt; Task: Initialize SBSA UART CAR I\u0026gt; Task: Initialize CPUBL Params I\u0026gt; CPUBL-params @ 0x1032000000 I\u0026gt; Task: Ratchet update W\u0026gt; Skip ratchet update - OPTIN fuse not set I\u0026gt; Task: Prepare eeprom data I\u0026gt; Task: Revoke PKC fuse I\u0026gt; PKC revoke fuse burn not requested I\u0026gt; Task: FSI padctl context save I\u0026gt; Task: Unpowergate APE W\u0026gt; mb2_unpowergate_ape: skip! APE is in unpowergated state I\u0026gt; Task: Memctrl reconfig pending clients I\u0026gt; Task: OEM firewalls I\u0026gt; OEM firewalls configured I\u0026gt; Task: Powergate APE I\u0026gt; Powergating APE I\u0026gt; Powergate done I\u0026gt; Task: OEM firewall restore saved settings I\u0026gt; Task: Unhalt AUXPs I\u0026gt; Unhalting SPE.. I\u0026gt; Enabling combined UART spe: early_init vic initialized tsc initialized aon lic initialized spe: tag is 524398t scheduler initialized aon hsp initialized tag initialized tcu initialized bpmp ipc initialized spe: late init cpu_nic clock initialized apb clock initialized pm initialized bpmp hsp initialized top1 hsp initialized ccplex ipc initialized spe: start scheduler I\u0026gt; Task: Trigger mailbox for PSC-BL1 exit I\u0026gt; Sending opcode 0x4d420802 to psc I\u0026gt; Received ACK from psc I\u0026gt; Task: Start secure NOR provision I\u0026gt; Skip Secure NOR provisioning I\u0026gt; Task: Trigger load FSI keyblob I\u0026gt; Skipping FSI key blob copy I\u0026gt; Task: Complete load FSI keyblob I\u0026gt; Skipping FSI key blob copy I\u0026gt; Task: MB2-PSC_FW Key Manager Init I\u0026gt; Sending opcode OP_PSC_KEY_MANAGER to psc-fw I\u0026gt; Sending opcode 0x4b45594d t hwwdt_init: WDT boot cfg 0x710010 sts 0x10 bpmp: socket 0 bpmp: base binary md5 is da583751bbfe2b7f6e204562d97ff39e bpmp: combined binary md5 is 39f77b2baaf3f0522607569dd3ae9a48 bpmp: firmware tag is 39f77b2baaf3f0522607-da583751bbf initialized vwdt initialized mail_early initialized fuse initialized vfrel initialized adc fmon_populate_monitors: found 199 monitors initialized fmon initialized mc initialized reset initialized uphy_early initialized emc_early initialized pm 465 clocks registered initialized clk_mach initialized clk_cal_early initialized clk_mach_early_config initialized io_dpd initialized soctherm initialized regime initialized i2c vrmon_dt_init: vrmon node not found vrmon_chk_boot_state: found 0 rail monitors initialized vrmon initialized regulator o psc I\u0026gt; Received ACK from psc I\u0026gt; Task: Unhalt FSI I\u0026gt; FSI unhalt skipped I\u0026gt; Task: Unhalt AUXPs I\u0026gt; Unhalting RCE I\u0026gt; RCE unhalt successful I\u0026gt; Unhalting DCE I\u0026gt; DCE unhalt successful I\u0026gt; APE unhalt skipped I\u0026gt; Task: Load HV/CPUBL I\u0026gt; Task: Load TOS I\u0026gt; Task: Trigger load TS[ 2.609271] Camera-FW on t234-rce-safe started initialized avfs_clk_platform initialized powergate TCU early console enabled. EC leyinitialized dvs initialized clk_mach_config suspend progress: 0x80000000 initialized suspend initialized strap initialized mce_dbell blob I\u0026gt; Sending opcode 0x53535452 to psc I\u0026gt; Sent opcode to psc I\u0026gt; Task: Load and authenticate registered FWs I\u0026gt; Partition name: A_cpu-bootloader I\u0026gt; Size of partition: 3670016 I\u0026gt; Binary@ device:3/0 block-24832 (partition size: 0x380000), name: A_cpu-bootloader DCE Started I\u0026gt; strt_pg_num(24832) num_of_pgs(16) read_buf(0x40067a30) I\u0026gt; cpubl : oem authentication of header done DCE_R5_Init I\u0026gt; strt_pg_num(24848) num_of_pgs(1) read_buf(0x102e143f98) I\u0026gt; strt_pg_num(24848) num_of_pgs(8) read_buf(0x102e143f98) I\u0026gt; cpubl : meta-blob integrity check isinitialized emc initialized emc_mrq MPU enabled success. I\u0026gt; strt_pg_num(24856)initialized clk_cal initialized uphy_dt initialized uphy_mrq HSIO UPHY reset has been de-asserted 0x0 num_oinitialized uphy f_pgs(512) r swdtimer_init: reg polling start w period 47 ms initialized swdtimer initialized hwwdt_late initialized bwmgr initialized thermal_host_trip initialized thermal_mrq initialized oc_mrq initialized reset_mrq initialized mail_mrq initialized fmon_mrq initialized clk_mrq initialized avfs_mrq initialized i2c_mrq initialized tag_mrq initialized bwmgr_mrq initialized console_mrq missing prod DT calibration data for 199 fmons initialized clk_sync_fmon_post DCE_SW_Init 80) I\u0026gt; strt_pg_num(25368) num_of_pgs(512) read_buf(0x10initialized clk_cal_late initialized noc_late initialized cvc 2e043f80) I\u0026gt; cpubl : will be decompreinitialized avfs_clk_mach_post initialized avfs_clk_platform_post initialized cvc_late initialized rm initialized console_late handling unreferenced clks enable can1_core enable can1_host enable can2_core enable can2_host enable pwm3 enable mss_encrypt enable maud enable pllg_ref enable dsi_core enable aza_2xbit enable pllc4_muxed enable sdmmc4_axicif enable xusb_ss enable xusb_fs enable xusb_falcon enable xusb_core_mux enable dsi_lp enable sdmmc_legacy_tm initialized clk_mach_post initialized pg_post [ 2.811175] Camera-FW on t234-rce-safe ready SHA1=e2238c99 (crt 12.427 ms, total boot 215.403 ms) initialized regulator_post initialized profile initialized mrq initialized patrol_scrubber initialized cactmon initialized extras_post bpmp: init complete ssed at 0x102c800000 I\u0026gt; version 1 Bin 1 BCheckSum 0 content_size 0 Content ChkSum 1 reserved_00 0 I\u0026gt; Reserved10 0 BlockMaxSize 5 Reserved11 0 I\u0026gt; strt_pg_num(25880) num_of_pgs(512) read_buf(0x102e083f80) I\u0026gt; strt_pg_num(26392) num_of_pgs(512) read_buf(0x102e0c3f80) I\u0026gt; strt_pg_num(26904) num_of_pgs(512) read_buf(0x102e103f80) I\u0026gt; strt_pg_num(27416) num_of_pgs(512) read_buf(0x102e003f80) I\u0026gt; strt_pg_num(27928) num_of_pgs(512) read_buf(0x102e043f80) I\u0026gt; strt_pg_num(28440) num_of_pgs(512) read_buf(0x102e083f80) I\u0026gt; strt_pg_num(28952) num_of_pgs(512) read_buf(0x102e0c3f80) I\u0026gt; strt_pg_num(29464) num_of_pgs(512) read_buf(0x102e103f80) I\u0026gt; strt_pg_num(29976) num_of_pgs(512) read_buf(0x102e003f80) Admin Task Init Admin Task Init complete Print Task Init RM Task Init SHA Task Init Admin Task Started I\u0026gt; strt_pg_num(30488) num_of_pgs(512) read_buf(0x102e043f80) DCE SC7 SHA Enabled RM Task Started RM Task Running Print Task Started Print Task Running SHA Task Started DCE: FW Boot Complete Admin Task Running Sf(0x102e083f80) I\u0026gt; cpubl : decompressed to 3661952 bytes I\u0026gt; cpubl: plain binary integrity check is success I\u0026gt; Partition name: A_secure-os I\u0026gt; Size of partition: 4194304 I\u0026gt; Binary@ device:3/0 block-32000 (partition size: 0x400000), name: A_secure-os I\u0026gt; strt_pg_num(32000) num_of_pgs(16) read_buf(0x40067a30) I\u0026gt; strt_pg_num(32016) num_of_pgs(3672) read_buf(0x103fd35000) I\u0026gt; MB2-params @ 0x40060000 I\u0026gt; NSDRAM carveout base: 0x80000000, size: 0xfacdf0000 I\u0026gt; cpubl_params: nsdram: carveout: 1, encryption: 1 I\u0026gt; cpubl: Authentication Finalize Done I\u0026gt; Binary cpubl loaded ne I\u0026gt; Binary tos loaded successfully at 0x103fd35000 I\u0026gt; Relocating OP-TEE dtb from: 0x103feff180 to 0x103c040020, size: 0x2754 I\u0026gt; [0] START: 0x80000000, SIZE: 0xfacdf0000 I\u0026gt; [1] START: 0x1E dtb finished. I\u0026gt; Partition name: A_eks I\u0026gt; Size of partition: 262144 I\u0026gt; Binary@ device:3/0 block-44288 (partition size: 0x40000), name: A_eks I\u0026gt; strt_pg_num(44288) num_of_pgs(16) read_bufc020000) I\u0026gt; eks: Authentication Finalize Done I\u0026gt; Binary eks lo0) @ VA:0x103c020000 I\u0026gt; Task: Add cpubl params integrity check I\u0026gt; Added cpubl params digest. I\u0026gt; Task: Prepare TOS params I\u0026gt; Setting EKB blob info to OPTEE dtb finished. I\u0026gt; Setting OPTEE arg3: 0x103c040020 I\u0026gt; NVRNG: Health check success I\u0026gt; NVRNG: Health check success I\u0026gt; Task: OEM SC7 context save I\u0026gt; OEM sc7 context saved I\u0026gt; Task: Disable MSS perf stats I\u0026gt; Task: Program display sticky bits I\u0026gt; Task: Storage device deinit I\u0026gt; Task: SMMU init I\u0026gt; Task: Program GICv3 registers I\u0026gt; Task: Audit firewall settings I\u0026gt; Task: Bootchain failure check I\u0026gt; Current Boot-Chain Slot: 0 I\u0026gt; BR-BCT Boot-Chain is 1, and status is 1. Set UPDATE_BRBCT bit to 1 I\u0026gt; Task: Burn RESERVED_ODM0 fuse I\u0026gt; Task: Lock fusing I\u0026gt; Task: Clear dec source key I\u0026gt; MB2 finished NOTICE: BL31: v2.8(release):e12e3fa93 NOTICE: BL31: Built : 17:14:28, Jan 7 2025 I/TC: I/TC: Non-secure external DT found I/TC: OP-TEE version: 4.2 (gcc version 11.3.0 (BuildrootTC: WARNING: This OP-TEE configuration might be insecure! I/TC: WARNING: Please check https://optee.readthedocs.io/en/latest/architecture/porting_guidelines.html I/TC: Primary CPU initializing I/TC: Test OEM keys are being used. This is insecure for shipping products! I/TC: fTPM ID is not enabled. I/TC: ftpm-helper PTA: fTPM DT or EKB is not available. fTPM provisioning is not supported. I/TC: Primary CPU switching to normal world boot Jetson UEFI firmware (version 36.4.3-gcid-38968081 built on 2025-01-08T01:18:20+00:00) I/TC: Reserved shared memory is disabled I/TC: Dynamic shared memory is enabled I/TC: Normal World virtualization support is disabled I/TC: Asynchronous notifications are disabled I/TC: WARNING: Test UEFI variable auth key is being used ! I/TC: WARNING: UEFI variable protection is not fully enabled ! [ 6.143531] Camera-FW on t234-rce-safe started TCU early console enabled. [ 6.205341] Camera-FW on t234-rce-safe ready SHA1=e2238c99 (crt 1.367 ms, total boot 63.240 ms) 3h ASSERT [XhciDxe] /out/nvidia/bootloader/uefi/Jetson_RELEASE/edk2/MdeModulePkg/Bus/Pci/XhciDxe/XhciSched.c(3154): Interval \u0026gt;= 1 \u0026amp;\u0026amp; Interval \u0026lt;= 16 Resetting the system in 5 seconds. Shutdown state requested 1 Rebooting system ... MMof crypto header is 8192 [0000.344] I\u0026gt; strt_pg_num(66816) num_of_pgs(16) read_buf(0x40050000) [0000.350] I\u0026gt; BCH of MEM-BCT-0 read from storage ... I/TC: Reserved shared memory is disabled I/TC: Dynamic shared memory is enabled I/TC: Normal World virtualization support is disabled I/TC: Asynchronous notifications are disabled I/TC: WARNING: Test UEFI variable auth key is being used ! I/TC: WARNING: UEFI variable protection is not fully enabled ! [ 6.145784] Camera-FW on t234-rce-safe started TCU early console enabled. [ 6.207503] Camera-FW on t234-rce-safe ready SHA1=e2238c99 (crt 1.357 ms, total boot 63.140 ms) 3h ASSERT [XhciDxe] /out/nvidia/bootloader/uefi/Jetson_RELEASE/edk2/MdeModulePkg/Bus/Pci/XhciDxe/XhciSched.c(3154): Interval \u0026gt;= 1 \u0026amp;\u0026amp; Interval \u0026lt;= 16 Resetting the system in 5 seconds. Shutdown state requested 1 Rebooting system ... 通過以上訊息，可以看到出現了 ASSERT 之後卡住，根據名稱判斷這是 UEFI 裡面的 xHCI USB 驅動 (Xhci) 崩潰。\n觸發 ASSERT 之後 5 秒強制重新開機，接著無限重新啟動，反映出的結果就是只看到黑畫面，永遠無法進入作業系統。從上面 Interval 判斷這是某個 USB 端點回傳了不合法輪詢的 Interval，或是被 UEFI 解析成非法的範圍，導致 ASSERT 觸發。\n關於合法的 Interval，查閱 USB 2.0 標準了解對 Isochronous 的 bInterval 在 Full-/High-Speed 常見的歸範圍 1 到 16，表示 $2^{(n-1)}$ 的 microframe 週期，中斷在 Full/Low-Speed 可以達到 1 到 255。\n詳細規格可以閱讀 USB 2.0 與 xHCI 1.1 的相關說明\n所以上面這個程式碼很可能是在判斷插入的 USB 2.0 裝置是否符合標準。如果不符合標準則觸發 assertion\n詳細解讀 boot log 根據 NVIDIA 官方文件，得知啟動流程如以下\nBootROM PSCROM MB1 MB2 UEFI 接著解讀以上啟動訊息\n首先可以看到最上面有大量 MB1/MB2 的訊息，包含載入 BPMP_FW, PSC_FW, RCE/DCE/XUSB 等等，屬於 Jetson Boot 啟動流程。SDRAM 初始化，時脈溫度監控等等，沒有看到錯誤訊息，所以在 MB1/MB2 階段沒有發生問題。\n接著看到一些警告，可能是來自 OP-TEE 的訊息，像是 Test UEFI variable，看起來像是開發用的測試 key 等等。\n在 MB1/MB2 之後，接著進入到 Jetson UEFI，UEFI 階段會載入內建的 xHCI 驅動 (XhciDxe) 掃描鍵盤，隨身碟等等週邊裝置，可能是為了提供 pre-boot I/O 或是從 USB 開機，通過 ASSERT 資訊判斷這是使用 edk2 框架進行開發的 Bootloader，可以看到進入到 UEFI firmware 之後就會崩潰 (判斷這個崩潰應該是在 UEFI 很早期的情況發生的，所以導致我們開機連 BIOS 的畫面都沒有看到)\n1 2 3 Jetson UEFI firmware (version 36.4.x ...) ASSERT [XhciDxe] ... XhciSched.c(3154): Interval \u0026gt;= 1 \u0026amp;\u0026amp; Interval \u0026lt;= 16 Resetting the system in 5 seconds. 我把這個 webcam 插到我的 fedora laptop 上，發現可以正常的開機，這邊我猜想 Linux Kernel 的 xHCI/USB driver 對 out-of-spac 的描述或許比較寬容，於是我查看 Linux Kernel xHCI/USB driver 的相關程式碼，希望把他移植到 edk2 UEFI 上，打上 patch，接著把 Jetson 上面的 Bootloader 抽換掉，解決這個問題。\n關於 Linux Kernel xHCI/USB 位於 drivers/usb/core/config.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 /* * Fix up bInterval values outside the legal range. * Use 10 or 8 ms if no proper value can be guessed. */ i = 0;\t/* i = min, j = max, n = default */ j = 255; if (usb_endpoint_xfer_int(d)) { i = 1; switch (udev-\u0026gt;speed) { case USB_SPEED_SUPER_PLUS: case USB_SPEED_SUPER: case USB_SPEED_HIGH: /* * Many device manufacturers are using full-speed * bInterval values in high-speed interrupt endpoint * descriptors. Try to fix those and fall back to an * 8-ms default value otherwise. */ n = fls(d-\u0026gt;bInterval*8); if (n == 0) n = 7;\t/* 8 ms = 2^(7-1) uframes */ j = 16; /* * Adjust bInterval for quirked devices. */ /* * This quirk fixes bIntervals reported in ms. */ if (udev-\u0026gt;quirks \u0026amp; USB_QUIRK_LINEAR_FRAME_INTR_BINTERVAL) { n = clamp(fls(d-\u0026gt;bInterval) + 3, i, j); i = j = n; } /* * This quirk fixes bIntervals reported in * linear microframes. */ if (udev-\u0026gt;quirks \u0026amp; USB_QUIRK_LINEAR_UFRAME_INTR_BINTERVAL) { n = clamp(fls(d-\u0026gt;bInterval), i, j); i = j = n; } break; default:\t/* USB_SPEED_FULL or _LOW */ /* * For low-speed, 10 ms is the official minimum. * But some \u0026#34;overclocked\u0026#34; devices might want faster * polling so we\u0026#39;ll allow it. */ n = 10; break; } } else if (usb_endpoint_xfer_isoc(d)) { i = 1; j = 16; switch (udev-\u0026gt;speed) { case USB_SPEED_HIGH: n = 7;\t/* 8 ms = 2^(7-1) uframes */ break; default:\t/* USB_SPEED_FULL */ n = 4;\t/* 8 ms = 2^(4-1) frames */ break; } } if (d-\u0026gt;bInterval \u0026lt; i || d-\u0026gt;bInterval \u0026gt; j) { dev_notice(ddev, \u0026#34;config %d interface %d altsetting %d \u0026#34; \u0026#34;endpoint 0x%X has an invalid bInterval %d, \u0026#34; \u0026#34;changing to %d\\n\u0026#34;, cfgno, inum, asnum, d-\u0026gt;bEndpointAddress, d-\u0026gt;bInterval, n); endpoint-\u0026gt;desc.bInterval = n; } USB 的 bInterval 因為不同速度或傳輸型別而有不同的意義。而部份設備廠商可能會錯誤的填入資訊，例如把 FS 的語意用在 HS 或是 SS，或是錯誤的寫成 microsecond, microframe 而不是規格要求的 2 的冪次數。\n上面的程式碼是為了容錯考慮，猜出一個正確值，或是給出一個合理的預設值，而不是直接報錯中止，如同 edk2 中做的那樣。\ni 表示合法的下限值 (min)，j 表示合法的上限值 (max)，n 表示預設值或是推測之後應該要設置的值。\n最下面的程式碼\n1 2 3 4 5 6 7 8 if (d-\u0026gt;bInterval \u0026lt; i || d-\u0026gt;bInterval \u0026gt; j) { dev_notice(ddev, \u0026#34;config %d interface %d altsetting %d \u0026#34; \u0026#34;endpoint 0x%X has an invalid bInterval %d, \u0026#34; \u0026#34;changing to %d\\n\u0026#34;, cfgno, inum, asnum, d-\u0026gt;bEndpointAddress, d-\u0026gt;bInterval, n); endpoint-\u0026gt;desc.bInterval = n; } 如果超出 min 或是 max，則會將 bInterval 設置為 n。\n所以目前的想法是希望可以放寬 Bootloader 的 bInterval，為此我們需要取得 Bootloader 的 source code，修改完成之後替換掉原先的 Bootloader。\n環境建置 1 2 3 4 # sudo systemctl enable --now docker # sudo groupadd -f docker # sudo usermod -aG docker $USER # newgrp docker 接著按照 nvidia-edk2 裡面的 wiki 進行建置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Point to the Ubuntu-22 dev image export EDK2_DEV_IMAGE=\u0026#34;ghcr.io/tianocore/containers/ubuntu-22-dev:latest\u0026#34; # Required export EDK2_USER_ARGS=\u0026#34;-v \\\u0026#34;${HOME}\\\u0026#34;:\\\u0026#34;${HOME}\\\u0026#34; -e EDK2_DOCKER_USER_HOME=\\\u0026#34;${HOME}\\\u0026#34;\u0026#34; # Required, unless you want to build in your home directory. # Change \u0026#34;/build\u0026#34; to be a suitable build root on your system. export EDK2_BUILD_ROOT=\u0026#34;/build\u0026#34; export EDK2_BUILDROOT_ARGS=\u0026#34;-v \\\u0026#34;${EDK2_BUILD_ROOT}\\\u0026#34;:\\\u0026#34;${EDK2_BUILD_ROOT}\\\u0026#34;\u0026#34; # Create the alias alias edk2_docker=\u0026#34;docker run -it --rm -w \\\u0026#34;\\$(pwd)\\\u0026#34; ${EDK2_BUILDROOT_ARGS} ${EDK2_USER_ARGS} \\\u0026#34;${EDK2_DEV_IMAGE}\\\u0026#34;\u0026#34; 程式碼修改與編譯 接著修改程式碼，這邊先採用最小可行性方案，以 Linux Kernel 的作法整體概念是猜一個數字放入到 bInterval，這裡我們仿造類似的方式，直接使用以下\n1 2 3 4 5 6 else if ((DeviceSpeed == EFI_USB_SPEED_HIGH) || (DeviceSpeed == EFI_USB_SPEED_SUPER)) { Interval = EpDesc-\u0026gt;Interval; - ASSERT (Interval \u0026gt;= 1 \u0026amp;\u0026amp; Interval \u0026lt;= 16); + if(Interval \u0026lt; 1 || Interval \u0026gt; 16) { + Interval = (Interval \u0026lt; 1) ? 1 : 16; } 強制讓 Interval 落於一個範圍內，先驗證這樣是否會造成問題，之後再將 Linux Kernel 的作法移入\n==TODO: 移植使用 Linux Kernel 的方案==\n修改完成之後執行以下開始編譯並產生出二進位檔案\n1 edk2_docker edk2-nvidia/Platform/NVIDIA/Jetson/build.sh --init-defconfig edk2-nvidia/Platform/NVIDIA/Jetson/Jetson.defconfig 二進位檔案會存在於 images 資料夾中，在資料夾中可以看到許多 .dtbo 以及 BOOTAA64_Jetson_RELEASE.efi 和 uefi_Jetson_RELEASE.bin，很明顯 .efi 以及 .bin 是我們主要需要的兩個檔案，根據 NVIDIA Jetson Linux Developer Guide 可以知道這一些檔案的用途\nBOOTAA64.efi 為 Jetson Linux OS Launcher UEFI Application，跟我們在 x86 會看到的 BOOTX64.efi 是類似的東西，放在 ESP (EFI System Partition) 上的 UEFI 應用程式，由 UEFI 韌體 (uefi*.bin) 呼叫，負責找到 Kernel, DTB, initrd 並把控制權交給 Linux。Jetson 預設 Loader 為 L4TLauncher uefi*.bin 為 Jetson 的 CPU BootLoader，為 UEFI firmware 的本體 (使用 EDK2 框架)，燒入在 Jetson 的 Bootloader Partition (在 Jetson 有 Slot A 以及 B，是為了在某一個 Bootloader 啟動失敗之後還能夠 rollback)。負責硬體初始化，載入各種 DXE 驅動，管理 UEFI NVRAM 變數，處理 Capsule 更新跟 A/B rollback，根據 Boot Order 去啟動 OS Loader，也就是上面說的 BOOTAA64.efi 會由 uefi*.bin 啟動 所以我們修改完 Xhci 相關程式碼之後，我們主要產生的是 uefi*.bin。\n接著我們需要 L4T，裡面包含打包/燒錄/更新工具等等，能處理 Jetson 機種, Partition Table, A/B slot 等等細節，幫我們把做好的 UEFI 與週邊裝置，如 DTB, DTBO, loader 組織成正確的 Image 或是 Capsule，接著通過 Jetson 內部工具更新 Bootloader。\n首先我們將 uefi*.bin 以及 BOOTAA64.efi 複製到 L4T 專案底下的 bootloader 資料夾中，接著開始打包\n1 # sudo ./l4t_generate_soc_bup.sh -e t23x_agx_bl_spec t23x 使用以上指令產生 BUP (Bootloader Update Payload)\n接著使用 BUP 產生 UEFI Capsule\n1 2 3 # ./generate_capsule/l4t_generate_soc_capsule.sh \\ -i bootloader/payloads_t23x/bl_only_payload \\ -o TEGRA_BL.Cap t234 得到 TEGRA_BL.Cap 之後將其放到 Jetson AGX Orin 中。\n在 Jetson AGX Orin 通過以下指令更新 Bootloader，新的 Bootloader 會被套用到非目前的 Slot，重開機之後我們會切換到該 Slot\n1 # sudo nv_bootloader_capsule_updater.sh -q TEGRA_BL.Cap 在開機的時候可以看到畫面最上方的 firmware 日期更新成我們新的 Bootloader 日期。\n而進入系統之後可以通過 sudo nvbootctrl --dump-slots-info 中 Capsule update status 的訊息，如果為 1 則表示更新成功。\n經過以上更新之後，順利進入到系統，解決之前系統黑畫面的問題。但是隨之而來又有新的問題產生\n1 2 3 4 5 6 7 [ 29.317410] usb 1-4.4: Failed to query (GET_DEF) UVC control 2 on unit 1: -32 (exp. 1). [ 29.318661] usb 1-4.4: Failed to query (GET_DEF) UVC control 2 on unit 1: -32 (exp. 1). [ 29.319911] usb 1-4.4: Failed to query (GET_DEF) UVC control 2 on unit 1: -32 (exp. 1). [ 29.321161] usb 1-4.4: Failed to query (GET_DEF) UVC control 2 on unit 1: -32 (exp. 1). [ 29.322426] usb 1-4.4: Failed to query (GET_DEF) UVC control 2 on unit 1: -32 (exp. 1). [ 29.323664] usb 1-4.4: Failed to query (GET_DEF) UVC control 2 on unit 1: -32 (exp. 1). ... 可以看到在 dmesg 中有大量的錯誤訊息，關於這部份將在後續檢查 UVC Driver 排除問題。總之目前能夠順利啟動並進入系統了。\n結論與整理 發現按下開機鍵沒有反應，通過風扇聲音判斷可能是在進入到 OS 之前就 Crash 了，通過 Micro-USB Debuf UART 得到 boot log 發現在 UEFI 階段 (XhciDxe) 在配置 USB 節點時，遇到裝置回傳不在範圍內的 bInterval，觸發 ASSERT，導致黑畫面以及卡住。這個範圍參考 USB 2.0 與 xHCI 1.1 相關描述 修改 edk2-nvidia 的 XhciSched.c (不是 ASSERT 裡面的路徑，那個是建制該專案電腦上的路徑)，將 bInterval 強制落於該範圍 將編譯完成的 UEFI 放入 L4T BSP，接著使用 l4t_generate_soc_bup.sh 產生 BUP，再用 lr4_generate_soc_capsule.sh 產生 TEGRA_BL.Cap 在裝置上使用 TEGRA_BL.Cap 完成 Bootloader 更新 參考資料，NVIDIA L4T 中的 README 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 # SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION \u0026amp; AFFILIATES. All rights reserved. # SPDX-License-Identifier: LicenseRef-NvidiaProprietary # # NVIDIA CORPORATION, its affiliates and licensors retain all intellectual # property and proprietary rights in and to this material, related # documentation and any modifications thereto. Any use, reproduction, # disclosure or distribution of this material and related documentation # without an express license agreement from NVIDIA CORPORATION or # its affiliates is strictly prohibited. The NVIDIA Public release Package provides a tool to update the QSPI flash partitions of Jetson devkits. This document provides information about updating the bootloader in the QSPI flash. 1. Prerequisites: - This tool is in the Public Release Package. It must be extracted to the Jetson Linux Package work directory (${Your_path}/Linux_for_Tegra/). 2. Preparation. - Download the Jetson Linux Package and make it ready for flashing. The work directory is \u0026#34;${Your_path}/Linux_for_Tegra/\u0026#34;. 3. Generate the QSPI flash bootloader payload. Here take the IGX as an example. a. Generate the BUP payload. $ cd ${Your_path}/Linux_for_Tegra/ $ sudo ./l4t_generate_soc_bup.sh -e t23x_igx_bl_spec t23x b. Pack the generated BUP to the Capsule payload. $ ./generate_capsule/l4t_generate_soc_capsule.sh \\ -i ./bootloader/payloads_t23x/bl_only_payload \\ -o ./Tegra_IGX_BL.Cap t234 Refer to the Jetson-Linux Developer Guide for more information about the Capsule payload. 4. Update the QSPI flash. a. Copy the generated QSPI flash payload to the target filesystem. $ scp ${Your_host_user_name}@${Your_host_IP}:${Your_path}/Linux_for_Tegra/Tegra_IGX_BL.Cap /opt b. Execute the bootloader_updater utility to update the IGX bootloader on QSPI flash. $ sudo nv_bootloader_capsule_updater.sh -q /opt/Tegra_IGX_BL.Cap c. Reboot the tareget to update the QSPI flash image on non-current slot bootloader. d. To check the Capsule update status, run the nvbootctrl command after boot to system: $ sudo nvbootctrl dump-slots-info Note: Capsule update status value \u0026#34;1\u0026#34; means update successfully. About the Capsule update status, please refer to developer guide for more information. e. To sync bootloader A/B slots, do the step b to d again. f. Then the bootloader partitions on QSPI flash(both A/B slots) are updated. 關於 DXE DXE 為 Driver Execution Environment，為 UEPI/PI 架構中的一個開機階段，參考下圖 在 PEI 之後，BDS (Boot Device Selection) 之前。這個階段由 Dispatcher 將各種 DXE 驅動載入到記憶體，初始化平台上大部分硬體，建立 UEFI 的 Boot Services 與整個 protocol, handler 等等，最後把系統交給 BDS 挑選並啟動 boot loader。\n以 xhciDxe 為例子，他是 DXE 驅動，負責在韌體時期把 xHCI USB 控制器 bring up，讓 UEFI 能夠使用 USB 鍵盤，從 USB 開機等等。\nCapsule Capsule 為一種帶有標頭簽章的韌體更新包，作業系統把他交給 UEFI 的 UpdateCapsule()，UEFI 依照 FMP (Fireware Management Protocol) 驗證與寫入指定韌體區域，通常會在下一次開機執行。\nCapsule 包含 header (Capsule GUID, size, flag)，後面加上 payload，多數平台使用 FMP Data Capsule，描述要更新的元件，版本以及 Image。UEFI 先使用 QueryCapsuleCapabilities() 檢查是否能處理，接著使用 UpdateCapsule() 申請更新。如果不使用 Jetson 裡面的腳本，或許也可以通過 fwnpdtool 進行 Bootloader 更新。\nCapsule 就是軟體更新包，將要更新哪個元件，版本以及 Image 使用標準格式描述並簽章，交給 UEFI 的 FMP 在下次開機安全寫入目標韌體區域，在 Jetson 開機時可以看到寫入階段。而上面的 TEGRA_BL.Cap 就是用來更新 bootloader/UEFI 的 Capsule。\n","date":"2025-09-10T23:00:12+08:00","permalink":"https://wuta0209.github.io/p/nv-jetson-debugging-update-bootloader/","title":"NV Jetson Debugging, Update BootLoader"},{"content":"在 Linux Kernel 2.6.28 合併了 Split LRU 的機制，並在後續版本中持續改進以及修復，直到 2.6.32 性能才變得穩定。而對於 tracking of recently reclaimed pages 還沒被整合，而是通過實作一種方法來動態調整 LRU list 的大小。\nLRU，為最常見的 page replacement 機制，在傳統實現中會將 user space 的 page 和 kernel space 的 page 都放在同一個 LRU list 中。這個做法在部分情況下會導致非最佳化的 page replacement 發生，特別是當 user space 的程式和 kernel space 爭搶記憶體時會影響到性能。\n2.6.27 之前 page replacement 問題 在 2.6.27 更早版本的 page replacement 存在兩大主要問題，這些問題在大型記憶體系統，如數 10 GB 甚至於上 100 GB 的系統中尤其嚴重。\nkernel 驅逐的 page 並非最佳的 page 甚至於不該驅逐的 page，導致性能下降 kernel 在回收記憶體時，可能會驅逐那些不該回收的 page，而保留應該被驅逐的 page 這會導致 cache thrashing 的問題，因為應用程式可能會頻繁重新載入被錯誤回收的 page 真正要被回收的 page cache 可能會被隱藏在大量 [[anon memory]] 或是 anon page 後面 Linux 使用 LRU list 管理記憶體，但 list 沒有區分 anon page 以及 page cache ( 用於硬碟 I/O 的 page ) 在沒有 Split LRU 之前，這些 page 被統一放在 LRU list 中，導致 kernel 在尋找可釋放的 page，可能會優先回收最容易找到的 page，而非最應該被回收的 page kernel 會掃描到不應該被回收的 page，導致 CPU 負載過高 在大型記憶系系統中，低效的 LRU 掃描只會增加 CPU 使用率 但在超大型記憶體系統中，這種掃描不但會導致 CPU 負載巨幅增加，甚至出現嚴重的鎖競爭問題，影響系統效能 kernel 不斷重複掃描 LRU list，其中許多的 page 根本不應該回收 不僅浪費 CPU 資源，還可能導致記憶體管理的 lock 競爭，拖垮系統 例如在一個有 80GB anon page 和 10GB page cache，沒有 swap 的系統中，在舊的 LRU 設計中\nkernel 會試圖去釋放 page cache (因為這一些 page 比較容易重新載入) 但是由於 page cache 被 80GB 的 anon page 擋住了，因此 kernel 需要不斷掃描 80GB 的 anon page，試圖找到可釋放的 page，結果就是 kernel 做很多無效掃描 在後續使用 Split LRU 嘗試解決上面問題，將 LRU 分成兩類\nFile-backed pages: 來自 disk 的 page，如 .txt, .so, .bin 等等 Anon pages: 應用程式的私有記憶體，如 Heap, Stack 當記憶體壓力大時，kernel 可以優先回收 File-backed pages，不需要掃描大量的 Anon pages。\nHigh-Level Overview Linux Kernel 的 VM 管理需要最小化掃描成本，確保 page replacement 在 best 和 worst 都可以高效率執行。Split LRU 的設計正式希望能夠解決這一些問題。\nTODO: page evict vs page reclaim vs page free vs page replacement\n核心設計概念 最小化掃描，避免無效的 page replacement 掃描成本隨著記憶體增長而增長，在面臨記憶體壓力下，kernel 應該只掃描可能需要釋放的 page，而不是整個 LRU list 大多數的 [[Page Churn]] 來自於讀取大型檔案。在大多數 server 的 workload 下，最主要記憶體占用來自於讀取大量檔案，例如 Database Systems, Log Processing, High-Frequency Trading, ML/AI Training 解決方案為優先驅逐這一些 file-backed pages，因為他們可以從硬碟重新載入，代價較低，避免掃描 anon pages，這一些屬於應用程式的私有記憶體，如 stack, heap 資料不能隨意丟棄 當記憶體不足時，才驅逐其他 page 當記憶體充足時，僅驅逐 file-backed pages。如果驅逐 anon pages，可能會有 swap，swap 對效能影響大 優先考慮 FileSystem I/O，而非 Swap I/O Swap 會有額外的硬碟寫入，影響性能 使用 Reference Count 和 Refault Data 平衡檔案快取以及 anon page Reference Count: 記錄一個 page 最近被 access 的頻率 Refault Data: 如果一個 page 在被驅逐之後又很快被載入，代表他應該被保留 根據這些資料動態調整 LRU，確保系統平衡 如果檔案快取 Refault Data 次數多，代表要保留更多檔案快取 如果 anon page Refault Data 次數多，代表要減少檔案快取，保留更多應用程式記憶體 關於 Best Case 和 Worst Case 的最佳化 正常情況: 優先驅逐 file-backed pages，減少沒必要的 page replacement 即便發生最壞情況，也不會發生因為過度掃描而發生 Lock Contention File Cache File Cache 主要負責儲存來自於硬碟，NFS, CIFS 的 data 或是 metadata，Linux Kernel 在處理這一些 File Cache 需要考慮以下幾點\n檔案系統大小遠大於記憶體 對於 Server 動則 TB 以上，記憶體比起硬碟小的許多。Kernel 需要動態管理 File Cache，確保最常用的 File Cache 儲存在記憶體中，而不是浪費記憶體儲存了一堆 Cold data 檔案存取特性 大多數檔案被存取的機率很低: 這一些資料通常是一次性讀取，如影片串流，備份與恢復，大規模資料處理，這些資料讀取完後不會重複存取 少量資料會被頻繁存取: 例如 database index, 應用程式 config, log 等等，因此這一些應該優先保留在記憶體中，避免 Page Fault 發生 使用 Used-Once 的 page replacement 對於 Used-once page replacement algorithm 的實現，Linux Kernel 使用兩條 LRU list 來對這一些 File Cache 進行管理 Inactive File List: 新載入的 File Cache 會放入 Inactive File List，如果只被存取一次，當它到達 List tail 的時候回收 Active File List: 當一個 page 被多次存取時，會從 Inactive List 遷移到 Active List，確保常用的 page 不會過早被驅逐 關於管理 Active List 與 Inactive List 如果 Inactive List 變太小，則將 Active List 中移動一些到 Inactive List: 當新的 File Cache 載入時，確保他們還有機會進入到 Active List 如果 Active List 大於 Inactive List / 2 的大小，則會將 Active List 部份元素開始降級，避免 hot 資料無限制的佔據記憶體 如何確保 hot 資料不會被驅逐 只要 Active List 佔據的記憶體大小不要超過 File Cache 總容量的一半，那麼這一些頻繁存取的 hot 資料將不會被 kernel 的 page replacement 逐出記憶體 關於 Active List 大小優化 可能可以考慮 Recently Evicted Pages 因素來調整 Active List 的大小 Anonymous Memory 為 Process 的私有記憶體空間，Shared Memory Segments 以及 tmpfs，這一些記憶體區段與 File Cache 不同，他們不直接對應到硬碟上的檔案，而是僅存在於 RAM 或是 swap 空間\n相較於 File Cache，數量較少: anon memory 通常和系統總記憶體大小相近，不像 File Cache 存在可能大於 RAM 容量的情況 由於 anon memory 主要存放程式的 stack, heap 等等，會比 File Cache 更加頻繁的被存取 通常不會被 swap out，除非記憶體嚴重不足。如果 kernel 頻繁掃描 anon memory，會導致效能損失以及記憶體管理效率下降 關於 Anon Memory 的管理其中一個方法為 SEQ replacement (Sequential Replacement)\n所有 anon page 一開始都是被引用的 無論 page 是否被引用，都會定從 Active List 移動到 Inactive 在 Inactive 的 page 被存取會躍遷到 Active 如果在 Inactive tail 的 page 還是沒有被存取，則會被 swap out Non reclaimable 被 mlock() 鎖定的 page 使用 shmget() 分配的共享記憶體，如果加上 mlock()，則不會被回收 ramfs, ramdisk 不會被換到 swap，也不會受到 LRU 回收機制影響，標記成 Non reclaimable Recently Evicted/ Nonresident 輔助 page fault 發生時的置換策略，以及調整 LRU List 大小。希望提供額外訊息，幫助 VM 更加好的進行 page replacement。不會儲存真正回收的 page，而是紀錄哪一些 page 最近被回收過。當發生 page fault 時，知道該 page 是否最近被回收過，進而決定是否優先分配回來，這個資訊可以避免 Thrashing 情況的發生。\n調整 LRU List 大小，例如當最近回收 page 比較多時，可以將 Active List 增大，減少頻繁的 swap out。\n因為我們儲存的是 meta data，考慮使用 radix tree 實做\n","date":"2025-09-02T11:51:13+08:00","permalink":"https://wuta0209.github.io/p/linux_pagereplacement_design/","title":"Linux_PageReplacement_Design"},{"content":"MemProf: a Memory Profiler for NUMA Multicore Systems source code: https://github.com/Memprof/module\n目標: 這篇論文是在開發 MemProf 工具，這個工具可以用於分析在 [[NUMA]] 架構底下的應用程式記憶體使用分析，可以提供應用程式在存取遠端記憶體時涉及的 [[thread]] 以及物件的資訊\n分析的應用程式: 四個應用程式，分別為 FaceRec, Streamcluster, Psearchy, Apache\n什麼樣的 workload 下 MemProf 表現得不錯? 具有任意生命週期的物件或是不同粒度大小的物件，或是應用程式對於記憶體的存取模式會隨著時間變化的，記憶體管理使用自定義策略的應用程式\n為什麼需要 MemProf: NUMA 相比起 UMA，會有記憶體存取的問題，如果我們嘗試存取不在目前 group 底下的記憶體，而是去存取其他 group 底下的記憶體，就會有顯著的時間開銷，換言之，對於 NUMA 架構下執行的應用程式，遠端存取記憶體的操作會嚴重的影響性能，舉例來說，在 node 2 上面執行的 process 去存取 node 1 的記憶體，這是論文中對於遠端存取的定義。而 group 的定義在 Linux 中 node 的定義是相同的，在傳統的 profiler 如 OProfile, Linux Perf, VTune, Memphis 等工具沒有提供關於 NUMA 記憶體存取的資訊，也就是應用程式在哪一個節點上執行，在某個時間點存取了什麼物件，且這個物件位於哪一個節點。\n雖然上面提及的這一些 profiler 有一些可以提供關於 global static memory object 的存取資訊，但是對於實際上的 workload 而言，這一些物件存取比率佔據所有記憶體存取事件中不到 4% 的比例，而對於非 global static memory object 的存取，上面提到的這一些分析器使用的方式是找一個目標記憶體地址，也許這個記憶體地址關連到某一個物件，然後追蹤這個記憶體地址涉及的存取指令。\n而 MemProf 相比起上面這一些工具，可以針對一個遠端的記憶體地址在被存取時所關聯的 thread 以及 object\nMemProf 實作部分 實作上依賴於 kernel module 用來蒐集資訊，以及使用一些 user space 上的 library 作一些資訊塞選，filiter 的操作。對於監聽記憶體存取事件，MemProf 使用 Instruction-Based Sampling (ISB) 的技術進行採樣\n定義應用程式對於 NUMA 性能負面影響的存取模式 第一種情況 (Remote usage after allocation) :當一個物件的記憶體是由 T1 所分配，分配的物件位於 N1 上，接著之後這物件被 T2 所存取，並在 N2 上面執行，這種存取模式通常會出現在具有生產者，消費者模型的應用程式中。要優化這個情況最簡單的方式就是直接把物件的記憶體分配在 N2 上，通常這種解法需要使用 NUMA-aware allocation functions 來達成，另外一種方式就是在 T2 開始存取資料之前通過 system call 去搬移資料，前提是不能夠帶來更大的 overhead 或是可以平均的分攤掉這個 overhead，像是 move_pages 第二種情況 (Alternate remote accesses to an object): 有多個 thread 隨時間變化去存取一個物件，以圖上的例子是有 T1, T2, T3 等等，而物件位於 N1 上，一個可以使用的作法是將 T1, T2, T3 綁定到 N1 上面執行，這一點可以用 CPU affinity 設定，如此可以避免 remote memory access，具體的作法為 thread 的 CPU affinity 設置只能在 N1 上面的 4 個核心上面執行，假設 N1 上面有 4 個核心，則 T1, T2, T3 嘗試存取該物件時，因為 thread 綁定在 N1 上，記憶體存取屬於本地存取。或是如果存取有一定的時間規律性，我們預測並根據時間預先對物件進行遷移 第三種情況 (Concurrent remote accesses to an object): 一個物件在一個時間段被多個執行序並行存取，如果我們使用之前的優化方式，直接將 T2 綁定在 T1 上面執行，可能會發生負載嚴重不平衡的情況。另外一種方式就是將該物件複製到多個記憶體節點上，但也會延伸出我們需要同步多個物件的問題，這個方法可能適用於唯獨物件中才能夠使用。如果 N1 的記憶體控制器已經飽和 (這一點可以通過評估計算記憶體的平均存取延遲時間來探測)，則可以考慮兩種最佳化的方式。第一個是平衡多個記憶體節點上不同最熱物件的分配，達成負載平衡，但是如果這樣的飽和情況是因為幾個大物件所引起的，則可可以將某一個這個 class 所分配的物件交錯分配在不同節點上，這樣優化可能可以降低記憶體存取延遲，或是盡可能的保持低延遲 在 NUMA 架構下的應用程式，主要會與性能相關的物件以及行為 全域靜態分配的物件 動態分配的物件 記憶體映射物件 作業系統映射的二進位檔案的部份，如某一些 binary 或是動態函式庫 thread 的 stacks Object lifecycle tacking 關於物件和 thread 的生命週期以及記憶體存取的追蹤方式，為 overload 記憶體相關分配函式，如 (malloc, calloc, realloc, free, mmap, mummap) 等等函式，如此就可以進行動態追蹤，具體的實做方式是通過 LD_PRELOAD 以及 dlsym 將要分析的應用程式與 MemProf 提供的 share library 連接所執行。\n對於 code sections 以及 global static variables 的生命週期追蹤，MemProf 實做的方式是通過 kernel module 進行處理，這個 kernel module 會 overload perf_event_mmap 這個函式，之所以選擇 overload 這個函式，是因為每一次 process 建立的時候，當 binary 或是 library 要映射到該 process 的記憶體空間時都會呼叫此函式。\n做 profiling 時，可以使用 kernel module 搭配 perf_event 系列的 API 去得到一些資訊\nAMD IBS 採樣 關於 overhead 主要開銷來自於 IBS 採樣頻率，對於採樣等等，需要去測量需要多少週期\n另外一個開銷來自於 trace user lib 和 kernel module trace 所帶來的開銷，攔截 user lib 並處理需要大約 400 的 clock，需要計算儲存這一些事件需要多大的緩衝區。\n關於 NUMA Profile 的相關工具 Memphis: NUMA 效能問題分析器，依賴於 IBS，主要找遠端存取的記憶體 VTune: 使用 PEBS，主要找遠端存取的記憶體地址 Dprof: 找到不良快取行為的物件 Oprofile 和 Perf: 使用 performance counter 去定位遠端存取的函式或是 asm code.\nLinux Kernel 裡面改進 NUMA 效能的 API 包含 cpusets 可以讓應用程式強制執行全域記憶體的策略，概念上就是強制硬程式在一有限的節點集合中分配記憶體，或是前面提到的 move_pages 可以用來做物件的遷移等等。\n在考慮記憶體遷移問題時，最好也順便遷移他的 working set。\n論文結論: NUMA 架構底下，應用程式中遠端存取記憶體會有大量的開銷，MemProf 可以在上面所敘述的 workload 底下表現良好，MemProf 的限制為很依賴於程式設計師對於問題的判定能力以及 MemProf 主要適合應用在 cache 效率低且大量記憶體存取的應用程式\n為什麼這一些 workload 足夠有代表性，這一篇論文在方法論上面有什麼創新? benchmark 是怎麼做的? 稍微掃過一次他的實作，感覺裡面有很多目前 eBPF 實作上使用到的技術\n","date":"2025-07-20T13:30:35+08:00","permalink":"https://wuta0209.github.io/p/memprof-a-memory-profiler-for-numa-multicore-systems/","title":"MemProf: A Memory Profiler for NUMA Multicore Systems"},{"content":"Coscup 2025: Let\u0026rsquo;s Tracing Linux Kernel, MGLRU 實作與分析 大綱 簡介，什麼是 page replacement? 回顧，關於 Active/Inactive LRU 追蹤，關於追蹤 kernel的 page replacement 介紹 MGLRU MGLRU call chain 解釋 Aging Eviction PID 控制 Rmap 優化 觀測 MGLRU 行為 結論 簡介，什麼是 page replacement? 首先回顧 Process 存取記憶體的流程\nCPU 存取記憶體: 當程式嘗試存取記憶體時，CPU 會先去查詢 TLB，如果 TLB Miss，則查詢 Page Table 查詢 Page Table: 如果發現 Page Table 沒有該虛擬記憶體對應到的實體記憶體映射，或是映射標記為無效，則 CPU 會觸發 Page Fault，產生 exception，並進入 Page Fault Handler 這時候有幾個選擇，如果 page 已經在記憶體，但沒有映射到目前 Process 的記憶體空間，則只需要更新映射就可以。如果 page 不在記憶體，需要從硬碟載入。如果 page 都不可使用，則系統需要選擇一個策略將目前實體記憶體的 page 進行替換，將某個 page 換出到次級記憶體，如 CXL, swap 等等，以釋放出空間給新的 page 使用，這時候可能使用 LRU 或是 MGLRU 等等策略 對於記憶體，主要問題有兩個，一個是什麼樣的物件應該保留在 cache 中，這個由 page replacement 策略決定，像是 MGLRU，而另一個議題是如何在 cache 中放入更多的物件，關於這部份近期的討論為 zram，不過我們會將重點放在 MGLRU。\n詳細關於 CPU 記憶體存取機制 當程式嘗試存取記憶體 (虛擬記憶體) 時，會進行以下\nCPU 查詢 TLB 確認虛擬記憶體地址是對應到哪一個實體記憶體地址\nTLB 查詢: CPU 先查詢 TLB，TLB 裡面儲存最近使用過得虛擬記憶體到實體記憶體之間的映射關係 TLB Hit: 如果在 TLB 中找到對應的映射，發生 TLB hit，則 CPU 直接使用該實體記憶體來存取記憶體 TLB Miss: 如果 TLB 中沒有找到對應的映射，則 CPU 需要先查詢主記憶體中的 Page Table 來獲取實體記憶體，這時候會需要進行 Page Table Walk 得到實體記憶體地址後，到 CPU cache 中查詢是否有實體記憶體地址對應的資料\nCPU Cache 檢查: CPU 首先檢查其內部 Cache，L1, L2, L3 Cache，如果資料已經被快取，則會發生 cache hit，直接使用該資料 Cache Miss: 如果 Cache 中沒有需要的資料，會發生 Cache miss，CPU 需要從主記憶體中讀取資料 關於 Page Table Page Table 中儲存虛擬記憶體到實體記憶體之間的映射關係，最後一層為 PTE (Page Table Entry)，包含以下幾個欄位\nP (Present Flag): 表示該 page 是否位於實體記憶體中 R/W (Read/Write Flag): 表示該 page 的讀寫權限 A (Accessed Flag): 表示 Page 是否被存取過 D (Dirty Flag): 表示 Page 是否被修改過 關於 Page Fault 當 Page Table 中沒有找到有效的映射，或是映射被標記為無效時，又或者是因為缺少權限等，CPU 會產生 Page Fault Exception，控制權會轉交給 Page Fault Handler，Page Fault 可以分成以下幾種類型\nMinor Page Fault 當 process 存取虛擬記憶體對應到的 page 已經存在於物理記憶體，但是尚未建立有效的映射到 Process Page Table，這時後會觸發 Minor Page Fault，這時候不需要硬碟 I/O，只需要更新 page table 的映射或是權限即可恢復執行 常見的為 malloc 中的 lazy allocation，CPU 發現虛擬記憶體地址沒有對應的實體 page frame，這時 kernel 分配 page frame 並更新 PTE，接著恢復程式執行，不需要硬碟 I/O Copy-On-Write (COW): 共享 page 首次寫入時觸發 copy 動態函式庫首次載入時，如果其 page 已經存在於 page cache，則會觸發 Minor Page Fault，kernel 只需要更新 page table 的映射，映射到 Process address space Major Page Fault: 當所需的 page 不在實體記憶體中，需要從次級記憶體，如硬碟, zswap, zram 等載入時，會發生 Major Page Fault，這種 Page Fault 處理需要消耗更多的時間，因為涉及 I/O 操作 從 zram 或是 swap 載入資料 Invalid Page Fault: 當 Process 存取的記憶體地址不在 Process 的虛擬記憶體空間中，或是違反存取權限，會發生 Invalid Page Fault。 關於 page replacement 當系統中 free page list 沒有足夠的 free page frame，這時候需要啟動 page replacement 機制。作業系統會根據以下策略選擇 victim page，並進行 eviction 操作，常見策略包含以下\nLRU: 替換最久沒有使用的 Page Clock / Second-Chance: 硬體成本較低的近似 LRU MGLRU (Multi-Generational LRU): 自 Linux Kernel 6.1 之後引入的策略，將 page 根據存活週期以及 hot, cold 分成多個 generation 整理以上，我們知道 page replacement 會在兩個情境下觸發\n記憶體壓力大時 (background reclaim): 當系統監測到目前記憶體壓力大時，如空閒記憶體空間低於某個特定水位，kswapd 會被喚醒，主動回收 page 維持系統可用的記憶體空間，主動選出哪一些 page 為 cold page 這部份會需要 page replacement 邏輯 由 page fault 觸發 (direct reclaim): 當發生 Major Page Fault，需要分配 page frame 時，如果 free page frame 不足，則會同步啟動 page replacement 機制，由 page fault handler 直接 evict 現有的 page，釋放空間給新的 page 使用 關於 Page Reclaim 在上面 Major Page Fault 中，提到當沒有足夠 free page frame 會需要啟動 page replacement 機制，Linux 會啟動 Page 回收機制，具體行為是由 (kswapd 或直接由 page fault handler 呼叫 shrink_node()) 嘗試釋放 page frame。回收時會根據 page 屬於的 LRU 層級，從 cold 資料優先選擇 victim page\nLRU page 分類 (會將 anon page 與 file-backed page 分開進行管理) inactive list: 使用頻率低的 page，優先被回收 active list: 近期有被存取的 page，回收之前 MGLRU: 將 page 更細分成多個 generation 如果記憶體壓力過大，kswapd 無法即時釋放得到足夠多的 free page frame，則會呼叫 direct reclaim，或是進行 memory compaction，合併可用的 free page frame，如果還是無法分配，則可能導致 OOM (Out-of-Memory) 機制觸發\n回顧，關於 Active/Inactive LRU 就是把 LRU 分成兩個 list，分別為 active list 和 inactive list，然後 anon page 和 file-backed page 互相獨立，再加上 unevictable page list，總共有 5 個 list\nInactive LRU list: 新載入或是剛剛使用的 page 一開始放入 Inactive list 中 (這一些 page 處於未活躍狀態) Active LRU list: 如果某個 page 在 Inactive list 中再次被存取 (該 page 第二次被使用)，則該 page 會被 promote 到 Active list，並且被標記成 Active page。Active LRU list 中的 page 表示最近常用或是 hot page 5 個 list 可以在 Linux Kernel 原始碼中看到\n1 2 3 4 5 6 7 8 enum lru_list { LRU_INACTIVE_ANON = 0, LRU_ACTIVE_ANON = 1, LRU_INACTIVE_FILE = 2, LRU_ACTIVE_FILE = 3, LRU_UNEVICTABLE = 4, NR_LRU_LISTS }; 而這一些 list 會存放在 lruvec 中\n1 2 3 4 struct lruvec { struct list_head lists[NR_LRU_LISTS]; ... }; 新的 page 預設會放入 Inactive LRU，當該 page 再次存取之後才會移動到 Active LRU 記憶體開始出現壓力之後會進行 Eviction，Eviction 的行為是從 Inactive LRU 的末端開始回收 如果 Inactive LRU 中 page 數量比較低，則從 Active LRU 尾端開始移動 page 到 Inactive LRU Inactive-Active LRU list 的運作: 當系統記憶體充足時，page 可以在兩個 LRU list 中移動。如果出現記憶體壓力，有 page 需要進行 Evict (Eviction: 將 page 從記憶體中移除)，Kernel 會從 Inactive LRU list 的末端開始回收不常使用的 page。這意味著只使用一次的那一些 page 會被優先淘汰，而 Active LRU list 中的 page 會被保留。\n如果 Inactive LRU list 長度太短，不足以提供 page 回收，則這時候會從 Active LRU list 末端選出一些 page 進行 demote 到 Inactive LRU list，增加 kernel 可以回收的 page 數量。\n雙 LRU list 設計目的: 這樣設計的核心概念是避免單次存取的 page 去干擾 working set，假想一下，如果只有一個 LRU list，由於 list 大小有限 (隱含 cache 大小有限)，那些只存取一次的 page 很有可能會把頻繁存取的 page 給擠出 LRU list，對整個程式的 working set 造成干擾。Active LRU list 相當於 working set 的保護區，Inactive LRU list 則是容納最近沒有重複使用的 page。\n雙 list 可以增加他的狀態表示數量 (包含 timestamp)，只有單 list，可以表示 hot/cold/dirty\n對於掃描模式，雙 list 可以讓他只污染到 inactive list (不過這部份要看 promote 邏輯)\n2Q lRU 與 active/inactive 是一樣的，MGLRU 動機: 為了提昇鑑別度，只有分兩層可能要掃描很久\n:::info working set 定義: 指一個應用程式在一段時間內正常執行所需要的時常存取的 page，也就是 hot page 所構成的集合 :::\n理想情況下 Active-Inactive LRU 表現 理想況下我們希望 Active-Inactive LRU 能夠很好的去避免那一些一次存取的 page 對一個應用程式的 working set 造成干擾。也就是 working set 中的 page 不會被 evict，我們希望有一個足夠長的 Inactive LRU 來避免對 working set 的干擾。\n舉一個例子，一個應用程式的 working set 大小為 6 個 page，而 Inactive LRU 長度為 8，接著開始存取 page，page 依序進入到 Inactive LRU，接著我們第二次存取 page，由於 Inactive LRU 長度大於 6，所以所有 page 都留在記憶體中，最終這一些 page 都可以被 promote 到 active list 中，也就是整個應用程式的 working set 順利保留在記憶體中。\nActive-Inactive LRU 可能的問題 Active-Inactive LRU 問題在理想情況下可以保護 working set，但在實際情況下存在以下不足\nRmap 開銷問題: 每次在回收或是檢查 page 的時候都需要通過 Rmap 找到相關的 page table entries，要知道這個 page 是被哪一些 process 所使用，這對 CPU 有著莫大的開銷 page 熱度探測: 必須等待該 page 再次被存取 (產生 page fault) 才能判斷該 page 為 hot page 並將其躍遷到 Active LRU list 分類簡單: 只有 Inactive/Active，也就是 page 不是 hot 就是 cold，分類上不夠細緻 上面的問題在非理想情況下 working set 可能會受到很大的影響，無法有效的保留在記憶體中，例如 working set 中 page 存取相對距離長，距離超過 Inactive LRU list 的大小，在 Active/Inactive LRU list 中可能會出現頻繁將 page 換出換入的情況 (Thrashing)。\n為了解決上面的問題，希望有個演算法能夠減少 Rmap 開銷並且提供更細緻的分類，為此，MGLRU 在 Linux Kernel 6.1 版正式引入\n補充: 關於 Rmap Rmap 為 Linux Kernel 中用於追蹤 Page Frame 被哪一些虛擬記憶體地址映射的機制，核心功能為以下\n反向查詢，給定一個 Page Frame，找出所有映射他的虛擬記憶體地址，也就是該 Page Frame 的使用情況，過程中需要遍歷 Process 的 Page Table 在 page 回收或是遷移時，需要更新所有相關 Process 的 Page Table，確保虛擬記憶體地址和實體記憶體地址之間的一致性 Rmap 在 Page Replacement 中通常會在以下兩個情況下觸發\nPage Reclaim: 當記憶體壓力大時，需要回收不常使用的 page，這時後會使用 Rmap 來找到哪一些 Process 正在使用該 Page Frame。接著會去更改這一些 Process 的 Page Table，移除這一些 Process 對於 Page Frame 的映射，讓 Page Frame 可以被釋放 Page Migration: 當需要進行 Page Migration，例如將 Page 移動到其他 NUMA 節點上面優化資料存取，這時候 Rmap 會用來更新所有映射該 Page Frame 的 Process 的 Page Table 中虛擬記憶體地址，使其指向到 Page Frame 新的位置 當 Kernel 需要回收一個 page 時，需要執行以下操作\n藉由 page replacement 機制選出 Victim Page，通常這個 Page 為 Cold Page 接著檢查 Page 狀態，如果是 Dirty 需要 write-back，如果是共享 Page，需要更新該 Page 關聯的所有 Process 的 Page Table 接著需要清除該 Page 相關的所有 PTE 中 Present 標誌，表示該 Page 已經不在記憶體中 Rmap 在上面共享 page 處理以及存取訊息蒐集時都需要進行對應呼叫\n如果 Page Frame 被多的 Process 共享，需要通過 Rmap 找到所有相關 PTE 並更新 檢查 PTE 的 Accessed, Dirty 等標記，輔助判斷 Page 的熱度 追蹤，關於追蹤 kernel 裡特定子系統 根據我們上面的推論，MGLRU 的整個流程會在兩個情況觸發\n當 page fault 發生時 當記憶體壓力大時，觸發 swap 機制時 也就是我們可以嘗試製造記憶體壓力大的場景，喚醒 kswapd 觸發 MGLRU 或是 LRU 的回收機制，具體可以使用以下三種作法\n使用 stress-ng 製造記憶體壓力 使用特定 workload，接著使用 cgroup 限制其能夠使用的記憶體資源 修改開機的 kernel-parameter，更改系統可以使用的記憶體大小，接著執行正常 workload 我們可以先使用 perf 去抓取 workload 期間的系統 call stack，得到 perf.data，接著對這個資料進行分析，得知 kernel 裡面某個 function 的執行流程，如 mglru\n使用 stress-ng 製造記憶體壓力 1 $ stress-ng --vm 4 --vm-bytes 123G --vm-keep --timeout 30s 使用測試程式，搭配 cgroup 限制記憶體資源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 int main() { size_t size = 110 * GB; void *buffer = NULL; size_t pagesize = get_page_size(); int ret = posix_memalign(\u0026amp;buffer, pagesize, size); FILE *fp = fopen(\u0026#34;va_pa_map.txt\u0026#34;, \u0026#34;w\u0026#34;); if (ret != 0) { fprintf(stderr, \u0026#34;posix_memalign failed: %s\\n\u0026#34;, strerror(ret)); return 1; } if (!buffer) { perror(\u0026#34;malloc\u0026#34;); return 1; } printf(\u0026#34;Allocating and touching 110 GB...\\n\u0026#34;); volatile unsigned long long counter = 0; for (size_t i = 0; i \u0026lt; size; i += pagesize) { counter++; volatile char *ptr = (char *)buffer + i; *ptr = 1; // touch page asm volatile(\u0026#34;\u0026#34; ::: \u0026#34;memory\u0026#34;); uintptr_t va = (uintptr_t)ptr; uint64_t pa = get_physical_address(va); if (pa == 0) { printf(\u0026#34;[USER] VA: 0x%lx -\u0026gt; PA: not mapped\\n\u0026#34;, va); fprintf(fp, \u0026#34;[USER] VA: 0x%lx -\u0026gt; PA: not mapped\\n\u0026#34;, va); } else { printf(\u0026#34;[USER] VA: 0x%lx -\u0026gt; PA: 0x%lx\\n\u0026#34;, va, pa); fprintf(fp, \u0026#34;[USER] VA: 0x%lx -\u0026gt; PA: 0x%lx\\n\u0026#34;, va, pa); } printf(\u0026#34;Counter = %d\\n\u0026#34;, counter); } printf(\u0026#34;Sleeping to allow kernel reclaim...\\n\u0026#34;); sleep(30); return 0; } 接著使用 cgroup 執行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash SLICE_NAME=appdemo.slice UNIT_NAME=alloc_$(date +%s) MEM_LIMIT=\u0026#34;300M\u0026#34; echo \u0026#34;[*] build systemd slice：$SLICE_NAME\u0026#34; # create slice, limit memory resource sudo systemctl set-property --runtime \u0026#34;$SLICE_NAME\u0026#34; MemoryMax=$MEM_LIMIT echo \u0026#34;[*] start running workload, running in slice\u0026#34; sudo systemd-run --unit=\u0026#34;$UNIT_NAME\u0026#34; --slice=\u0026#34;$SLICE_NAME\u0026#34; --pty --same-dir ./alloc echo \u0026#34;[*] workload finish, try cleanup\u0026#34; # cleanup sudo systemctl stop \u0026#34;$UNIT_NAME\u0026#34; sudo systemctl reset-failed \u0026#34;$UNIT_NAME\u0026#34; sudo systemctl stop \u0026#34;$SLICE_NAME\u0026#34; CGROUP_PATH=\u0026#34;/sys/fs/cgroup/memory/$SLICE_NAME\u0026#34; if [ -d \u0026#34;$CGROUP_PATH\u0026#34; ]; then echo \u0026#34;[*] waiting for cleanup\u0026#34; while [ -s \u0026#34;$CGROUP_PATH/cgroup.procs\u0026#34; ]; do sleep 0.5 done echo \u0026#34;[*] remove cgroup：$CGROUP_PATH\u0026#34; sudo rmdir \u0026#34;$CGROUP_PATH\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34;Error\u0026#34; fi echo \u0026#34;[*] Done\u0026#34; 關於 flamegraph 與 dot-graph flamegraph 為視覺化工具，用來顯示程式在執行期間的 stack traces，可以用來得知某個系統的效能瓶頸以及分析函式呼叫關係。以 Linux 中的使用，我們可以將 perf.data 作為後端資料，接著通過前端腳本對 perf.data 進行過濾之後得到 flamegraph。\n以下為通過 perf report 檢視的部份原始資料\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kswapd0 73 [002] 4400339479.836174: 15575919 cycles:P: ffffffff8122b313 shrink_folio_list+0x143 ([kernel.kallsyms]) ffffffff8122f903 evict_folios+0x153 ([kernel.kallsyms]) ffffffff81231285 lru_gen_shrink_node+0x125 ([kernel.kallsyms]) ffffffff81231f1c balance_pgdat+0x29c ([kernel.kallsyms]) ffffffff81232593 kswapd+0x1e3 ([kernel.kallsyms]) ffffffff810c1bf7 kthread+0xd7 ([kernel.kallsyms]) ffffffff810441fc ret_from_fork+0x3c ([kernel.kallsyms]) ffffffff8100267a ret_from_fork_asm+0x1a ([kernel.kallsyms]) kswapd0 73 [002] 4400339479.836174: 15575919 cycles:P: ffffffff81225938 folio_update_gen+0x38 ([kernel.kallsyms]) ffffffff8127486a walk_pgd_range+0x24a ([kernel.kallsyms]) ffffffff812750eb __walk_page_range+0x19b ([kernel.kallsyms]) ffffffff8127525c walk_page_range+0x14c ([kernel.kallsyms]) ffffffff8123049f try_to_inc_max_seq+0x46f ([kernel.kallsyms]) ffffffff81231110 get_nr_to_scan+0x60 ([kernel.kallsyms]) ffffffff8123126b lru_gen_shrink_node+0x10b ([kernel.kallsyms]) ffffffff81231f1c balance_pgdat+0x29c ([kernel.kallsyms]) ffffffff81232593 kswapd+0x1e3 ([kernel.kallsyms]) ffffffff810c1bf7 kthread+0xd7 ([kernel.kallsyms]) ffffffff810441fc ret_from_fork+0x3c ([kernel.kallsyms]) ffffffff8100267a ret_from_fork_asm+0x1a ([kernel.kallsyms]) 以下為 perf.data 過濾之後得到的 flamegraph\n1 2 3 4 5 $ sudo perf record -F 99 --call-graph dwarf -ag -- sleep 60 # running workload $ sudo perf script \u0026gt; mglru_trace.perf $ ./FlameGraph/stackcollapse-perf.pl mglru_trace.perf \u0026gt; mglru_folded.txt $ ./FlameGraph/flamegraph.pl --title=\u0026#34;MGLRU Memory Reclaim\u0026#34; --width 2000 --colors mem mglru_folded.txt \u0026gt; mglru_flame.svg :::info 使用 DWARF 可以提供更多 debug-info，詳細可以參考 Kernel Recipes 2017 - Perf in Netflix - Brendan Gregg :::\n可以看到數據非常的龐大，這時候我們需要對我們感興趣的部份進行二次過濾，我們感興趣的目標為 mglru，而我們上面推論得到 mglru 會在記憶體壓力大時執行，Linux 在記憶體壓力大時會觸發 swap 機制，而 swap 機制是由 kswapd 執行，因此我們可以過濾出關於 kswapd 的資料來得到 mglru 的 call stack\n1 $ grep -E \u0026#34;kswapd0\u0026#34; mglru_folded.txt | ./FlameGraph/flamegraph.pl --colors mem --width 2000 --title=\u0026#34;MGLRU\u0026#34; \u0026gt; mglru_specific.svg 從上圖我們就可以清楚的看到整個 mglru 的 stack traces。上面過濾出的，是針對記憶體壓力大時，觸發 swap 機制。\n由 kswapd 的圖我們可以看出存在兩條主要路徑，分別為\n路徑1. kswapd -\u0026gt; balance_pgdat -\u0026gt; shrink_node -\u0026gt; shrink_one -\u0026gt; try_to_inc_max_seq -\u0026gt; walk_page_range\n路徑2. kswapd -\u0026gt; balance_pgdat -\u0026gt; shrink_node -\u0026gt; shrink_one -\u0026gt; try_to_shrink_lruvec\n下面我們也可以針對 page fault 進行過濾，得到以下 flamegraph\n1 $ grep -E \u0026#39;lru|folio|shrink|refault\u0026#39; mglru_folded.txt | head -n 20 \u0026gt; page_fault_info_mglru.txt 由 page fault 的圖我們可以看出一條主要路徑 路徑3. exec_page_fault -\u0026gt; do_fault -\u0026gt; filemap_fault -\u0026gt; lru_add -\u0026gt; lru_gen_add_folio\n由 flamegraph 分析 mglru，我們得到 mglru 存在三條主要 stack straces 路徑，作用分別為以下 作用分別為以下作用分別為以下\n路徑1. Promote/Aging (hot page 升級，產生新的 gen) 路徑2. Evict / Refault (用於 page 淘汰) 路徑3. Page Fault (將 page 加入到最新 gen，也就是 max_seq) 對於路徑分析，我們也可以使用 dot-graph 進行分析，可以更加具體的看到 function 之間的呼叫關係 1 2 $ cat mglru_trace.perf | FlameGraph/stackcollapse-perf.pl \u0026gt; out.collapse $ gprof2dot.py -f collapse out.collapse | dot -Tpng -o output.png MGLRU 描述 MGLRU 核心想法是將 page 按照熱度分層管理，但相比起 Inactive/Active LRU list，MGLRU 將 page 區分成多個 generation。每個 page 對應到一個 gen 值，表示該 page 屬於哪一個 geneartion。\n以下為 MGLRU 中關鍵部份，大致上可以分成以下五個部份\nGeneartion 概念: page 兩種分類，gen 以及 tier Rmap 遍歷，得知一個 page/folio 是否為 young Page Table Walk: 查詢 Page Table 查看 PTE 是否為 young Bloom Filter: 縮小 Page Table Walk 需要走訪的範圍 PID 控制器: 用來控制回收的 page 類型以及保護 page Geneartion 概念 MGLRU 分類 page 主要使用上面 Gen 以及 Tier，Gen 和 Tier 為不同的概念，以下詳細描述\n世代 (Gen):\nGen 意義: 每個 page 屬於一個 gen，表示 page 的熱度或是年齡。使用 sliding windows 實作 Gen 計算: gen 是由 min_seq 與 max_seq 定義，通過 seq % MAX_NR_GENS 計算出 gen Gen 分類: max_seq 表示最年輕的 gen，min_seq 表示最老的 gen youngtest gen: max_seq (最年輕，熱度最高) young gen: max_seq - 1 old gen: min_seq + 1 oldest gen: min_seq (最老的，熱度最低) Gen 維護: 對於 max_gen，anon page 和 file-backed 視為相同的 hot page，但是 anon page 和 file-backed 會分別維護 min_seq Promote 機制與 Demote 機制\nPromote: 當 page 被再次存取時會 Promote 到新的 gen Demote: 在 MGLRU 中不存在顯式 Demote 的操作，當 max_seq 增加時，沒有 Promote 的 page 相對年齡就增加了，達到隱式 Demote 的效果。 page 沈降: 隨著時間推移，沒有 Promote 的 page 自然變成 oldest gen，變得可能被回收的 page。當 oldest gen 中的 page 被全部回收，LRU list 被刪除並且 min_seq++，整個 windows 往前滑動 generation 不足: 當 generation 數量不足時，則會建立新的 generation 繼續區分 page 的熱度 層級 (Tier) 定義: 每個 page (folio) 根據 (通過 fd 存取的次數，像是 write(), read()) 被分配到一個 Tier，假設 page 通過 fd 存取的次數為 $N$，則 $Tier = order_base_2(N)$。每個 Page 使用額外 2 bit 去紀錄他們屬於的 Tier，Tier 的最大值為 $MAX_NR_TIERS = 4$\nTier = 0 -\u0026gt; N = 0,1 Tier = 1 -\u0026gt; N = 2,3 Tier = 2 -\u0026gt; N = 4,5,6,7 Tier = 3 -\u0026gt; N = 8+ (上面這個使用圖表進行表示) Refault 率統計\nRefault 定義: 某個 page 被回收後再次被存取 (該 page 重新被載入到記憶體) 的比率 Refault 資料結構: 統計方式為通過 lrugen-\u0026gt;refaulted[hist][type][tier] 記錄不同世代 (hist)，Page 類型 (anon/file-backed), Tier 的 refault 次數。使用這個陣列紀錄每一個 Tier 的 refault 率，也就是這個 Tier 被回收的 page 之後又被重新載入 (refault) 的比例。 Refault 率 = 被回收又重新載入的 page 數量 / 被回收的 page 數量 決策機制: 先比較 min_seq 的值，選擇較小值，也就是更老的 page 類型 如果兩個 page 類型都相同，則比較 Tier 0 的 refault 率，選擇比較低的 page 類型進行回收 使用 PID 控制器的回饋機制，動態平衡不同 page 類型的 refault 率 總結一下，每個 page 通過額外的 flags 紀錄自己屬於的 Tier，系統會維護額外的 array 去紀錄每一個 Tier 的 refault 率。eviction 策略會根據 refault 動態調整回收的行為，避免頻繁存取的 page 但是未達到 Promote 標準的 page 被過早的回收，也就是 refault 率可以用來判斷當兩種 page 類型年齡相同時，refault 率可以用來判斷要回收的 page，更加精細化 page 的分類。\n由上面的描述，我們會發現到，當有一個 Page 位於最老的 gen，但是他的 refault 率高，這時候我們應該有一個機制對這類的 page 進行處理，避免他們被回收，影響到 working set，在 MGLRU 中存在 Protect 機制用來保護這一些 page。\nProtect 機制: 當某個 Tier 的 Refault 率高於 Tier 0 的 Refault 率，將會觸發 Protect 機制 觸發 Protect 機制的 page，會被移動到 old gen min_seq + 1，相當於給這個 page second chance，避免整個 oldest gen 一次被回收乾淨，造成部份 Trashing 現象發生 目標為了提高 MGLRU 對於密集 I/O 的 working set 保護效果 以下為 mglru 關鍵資料結構程式碼\n1 2 3 4 5 6 7 8 9 10 struct lru_gen_folio { /* the aging increments the youngest generation number */ unsigned long max_seq; /* the eviction increments the oldest generation numbers */ unsigned long min_seq[ANON_AND_FILE]; /* the birth time of each generation in jiffies */ unsigned long timestamps[MAX_NR_GENS]; /* the multi-gen LRU lists, lazily sorted on eviction */ struct list_head folios[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES]; ... 存放 page (folios) 為一個三維陣列結構，使用以下方式進行拆分\n第一維度: Generation，最多 4 個 generation 第二維度: 屬於 anon page 還是 file-backed page 第三維度: NUMA Node，對於 UMA 為 1 記錄 Refault 率的結構也同樣位於 lru_gen_folio 中\n1 2 3 4 5 6 7 8 9 10 11 12 struct lru_gen_folio { /* the multi-gen LRU sizes, eventually consistent */ long nr_pages[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES]; /* the exponential moving average of refaulted */ unsigned long avg_refaulted[ANON_AND_FILE][MAX_NR_TIERS]; /* the exponential moving average of evicted+protected */ unsigned long avg_total[ANON_AND_FILE][MAX_NR_TIERS]; /* can only be modified under the LRU lock */ unsigned long protected[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS]; /* can be modified without holding the LRU lock */ atomic_long_t evicted[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS]; atomic_long_t refaulted[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS]; 下圖簡單描述 gen, tier 之間的關係 下突圍 Protect 與 Promote 之間的關係 路徑1. Aging 機制 所謂 Aging，指的是通過建立新的 gen 提高 page 熱度判斷的機制。在 kernel 回收 page 的時候，當發現現有的 gen 數量不足以區分 hot/cold page 時，這時後會觸發 Aging 操作，Aging 操作會新增一個 gen。\n1 2 3 4 5 6 7 8 9 10 11 kswapd() └─ balance_pgdat() └─ shrink_node() └─ shrink_one() └─ try_to_shrink_lruvec() └─ lru_gen_shrink_lruvec() └─ should_run_aging() └─ try_to_inc_max_seq() └─ lru_gen_age_node() └─ walk_page_range (for each mm_struct) └─ folio_update_gen() 啟動 Aging 後，MGLRU 會通過 Page Table Walker 更新 page 的熱度，接著對於目前記憶體中每個 process/mm_struct，會呼叫 walk_page_range() 遍歷其 Page Table，以找到年輕的 page (PTE 的 Accessed bit 為 1 的)。為了降低開銷，遍歷之前會先查詢 Bloom Filter 找到值得掃描的 Page 區域 (例如包含多個 hot page 的 PMD)。在遍歷過程中，如果發現這個範圍內存在 Access Bit 為 1 的 PTE，表示該 page 近期有被存取過，屬於潛在的 hot page，這時後會通過 folio_update_gen() 標記該 page promote 到最新的 gen，也就是將該 page 的 generation 更新成 max_seq，而 promote 操作是只去修改 page 的 gen 標記，不是立即移動該 page 在 LRU list 中的位置，直到真正 evict 的時候才會進行移動\n當一輪 Aging 的操作結束，新的 gen 產生，這時候 max_seq + 1，並紀錄目前的 timestamp。MGLRU 隨後會使用該 timestamp 實現 Thrashing 防護，如果之後嘗試回收 min_seq 裡面的 page，發現到 min_seq 的產生時間還沒超過預設的 TTL 閥值，則會暫緩回收該 gen。這個機制避免系統頻繁的回收/建立 gen，防止 working set 因為頻繁的在 gen 之間移動造成 Thrashing。\n完成 Aging 之後，MGLRU 會繼續進行 page evict 的流程，不斷循環的執行 Promote / Evict / Aging 的流程，使得 hot page 保留在最新的 gen，cold page 沈積到最老的 gen。在任何時刻，MGLRU 最多保持 4 個 gen，這一件事情是通過 sliding windows (4 \u0026gt;= max_seq - min_seq \u0026gt;= 2) 保證。\n以 call chain 進行分析，則可以得到以下\n記憶體高壓，需要將 page swapout，這時候 kswapd 被喚醒，接著執行 shrink_node() -\u0026gt; shrink_lruvec() -\u0026gt; lru_gen_shrink_lruvec -\u0026gt; should_run_aging -\u0026gt; try_to_inc_max_seq() 執行主要的 Aging -\u0026gt; 內部對每個 memcg 的 mm_list 呼叫 walk_page_range() 遍歷 Page Table，在 callback 中對遍歷 Page Table 過程中發現的 young page (PTE 為 1) 的 page 呼叫 folio_update_gen() Promote 該 page 的 gen，同時將資訊回饋到 Bloom Filter 優化值得走訪的記憶體區域，全部完成之後準備進行 Eviction\n整體流程為以下 路徑2. Evict :::info 關於 walk_page_range() 與 rmap 的差別\n:::\nEviction 流程指的是從 min_seq 開始淘汰 page 的機制。在 MGLRU 中，始終只回收目前 min_seq LRU list 中的 page。當觸發 kswapd 時，kernel 會選擇 min_seq 進行掃描以及回收。由於 MGLRU 會將 hot page 通過 Aging 機制將 hot page Promote 到 max_seq，自然而然相對 cold page 就會沈降到 min_seq，min_seq 裡面的 page 為長時間沒有存取的 page，可作為回收的目標。對於 min_seq，MGLRU 會分別維護 anon page 和 file-backed page 兩個 list，因此可能存在 anon page 和 file-backed page 兩者 min_seq 不同步的情況。\n以下為 Eviction 的 call chain\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kswapd() └─ balance_pgdat() └─ shrink_node() └─ shrink_one() └─ try_to_shrink_lruvec() └─ evict_folios() ├─ isolate_folios() │ └─ scan_folios() │ ├─ sort_folio() // Promote 熱頁到次老世代 │ └─ isolate_folio() // 選出 page ├─ shrink_page_list() // 對選出的 page 進行實際回收 │ ├─ folio_check_references() │ │ └─ folio_referenced() │ │ └─ lru_gen_look_around() // 局部性優化，尋找附近的 hot PTE │ ├─ lru_gen_set_refs() // 決定是否 Protect（移動到 min_seq+1） │ └─ try_to_free_swap() / folio_free() // swap out 或釋放 └─ try_to_inc_min_seq() // 若 oldest gen 已清空則淘汰並 min_seq++ 進入 Eviction 流程後，關鍵函式 lru_gen_shrink_lruvec() 按照一定策略從 min_gen list 提取 page 進行處理。每次提取一批 page 後，會對這一些 page 逐一進行回收判斷。對每個 page (struct folio)，首先呼叫 folio_check_references() 檢查最近是否被引用或是屬於 working set 的 page，folio_check_references() 會進一步呼叫 folio_referenced() 執行 Rmap 走訪，查詢該 page 映射的所有 PTE，統計該 page 的引用情況。\n關於 folio_referenced 的實現運用到空間局部性進行優化，當通過 Rmap 找到某個 process 的 PTE 值，MGLRU 會呼叫 lru_gen_look_around 查看該 PTE 相鄰的多個 PTE，如果附近的 PTE 也屬於年輕的 (也就是 Access Bit 為 1)，則這一些 page 會被標記為 Promote，之後便會真正 Promote 到 max_seq。利用 Look-around 機制，MGLRU 可以在一次 Rmap 中找到多個 hot page，減少重複遍歷 Page Table 的開銷，並且這些 PTE 對應到的上層 Page Table 節點，也就是 PMD 會被紀錄到 Bloom Filter 中，做為之後 Aging 階段的輔助判斷資訊\n對於每個從 min_seq 提取的 page，folio_check_references 會根據 rmap 的結果回傳 enum，該 enum 的意義是對 page eviction 的建議\nFOLIOREF_RECLAIM: page 最近沒有被 reference，可以回收 FOLIOREF_ACTIVATE: page 最近被 reference，應該 Promote 並保留 FOLIOREF_KEEP: 暫時不處理 (可能正在被其他程式使用或 lock) 如果 page 最近沒有任何引用，也就是 referenced_ptes 為 0，則屬於真正的 cold page，將按照該 page 屬於的類型進行 direct reclaim。屬於 file-backed page 會直接回收或是 write-back 到硬碟，而如果屬於 anon page 則會 swap 出去。\n如果 page 最近有使用，則 MGLRU 會給該 page 第二刺激會，folio_check_references() 通過 lru_gen_set_refs() 函式判斷是否需要 promote 該 page，這裡具體為使用 Tier 去判斷該 page 是否最近被使用過以及是否屬於 working set (也就是被 evict 之後又重新載入的 page，refault 的概念)\n第一次發現引用: 如果 page 此前沒有標記 Referenced 也不是屬於 working set 的 page，lru_gen_set_refs() 將該 page 標記為 referenced (設置 PG_referenced Flag)，接著回傳 false，意義為該 page 暫時不做 Promote。整個意義是紀錄該 page 引用了一次，給 page 一次機會留在目前的 gen，下一次再遇到該 page 的時候再做決定 再次被引用或是曾經被 refault: 如果 page 已經有 referenced 標記或者其 PG_workingset Flag 為 True (表示該 page 曾經 refault 過，屬於 working set 的一部分)，那麼 lru_gen_set_refs 會回傳 True，表示該 page 需要進行 Promote。這時候 folio_check_references() 會回傳 FOLIOREF_ACTIVATE，kernel 會將該 page 從 min_seq 移除並重新加入到 max_seq，實作為移動到 generation 最高的 list，避免這次被回收，這個提升 page 的操作也稱為對該 page 的 Protect 上面的策略，MGLRU 確保一個 page 要被 Protect，也就是 page 被 Promote 需要符合以下一個條件\nPage 被存取兩次 該 Page 屬於 working set 通過以上策略避免一些 page 因為一次偶然的存取就阻止該 page 被回收。而對於確實經常被存取的 page，MGLRU 會對該 page 進行 Protect 避免其被淘汰 隨著 eviction 機制進行，min_seq 裡面的 page 不是被回收釋放，不然就是因為他有引用而被移除 min_seq加入到 max_seq。Kernel 會持續進行上面的操作，直到 gen 裡面所有 page 處理完畢，接著該 gen 就會被丟棄，將 min_seq + 1 (淘汰最老的 gen)，這部份由 inc_min_seq() 完成，負責更新 anon page 和 filed-back page 對應的 oldest gen seq 並清理相關資料結構。丟棄 gen 的同時，kernel 會去維護一些輔助資料結構，像是該 gen 中被回收的 page 數量，各 Tier 被回收和 refault 次數等等，特別會針對剛剛在回收或成因為 refault 率高而被 Protect 的 page 資訊也會被記錄下來，納入歷史統計中，用於調整後續的 eviction 策略。完成一個 gen 回收後，如果記憶體壓力沒有紓解並且需要釋放更多記憶體，MGLRU 會繼續重複上面 Aging + Eviction 過程\n以 call chain 進行分析如下 lru_gen_shrink_lruvec() -\u0026gt; 從 min_seq list 中提取 page 進行處理 -\u0026gt; 對每個 page 呼叫 folio_check_references() 檢查是否被引用 -\u0026gt; folio_eferenced() 執行 rmap 查詢 Access Bit (裡面呼叫 lru_gen_look_around) 檢查鄰近 PTE 並標記 hot page，更新 Bloom Filter) -\u0026gt; 根據 folio_check_references 回傳的 enum 決定 folio 應該如何處理，沒有引用的進行回收 (呼叫 try_to_free_swap 或 folio_free 釋放)，如果有引用則呼叫 lru_gen_set_refs 判斷是否應該 Promote (進行 Protect) -\u0026gt; 如果需要 Promote 則將該 page 加入到 max_seq -\u0026gt; 繼續處理下一個 page，直到提取出的 page 都被掃描完畢 -\u0026gt; 處理完畢後呼叫 inc_min_seq 將該 gen 丟棄，並將 min_seq + 1\n整體流程為以下 整個 Eviction 和 Aging 和 Promote 行為一個循環圖，可以表示成以下 關鍵函式 try_to_shrink_lruvec 分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 static bool try_to_shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc) { long nr_to_scan; unsigned long scanned = 0; int swappiness = get_swappiness(lruvec, sc); while (true) { int delta; nr_to_scan = get_nr_to_scan(lruvec, sc, swappiness); if (nr_to_scan \u0026lt;= 0) break; delta = evict_folios(lruvec, sc, swappiness); if (!delta) break; scanned += delta; if (scanned \u0026gt;= nr_to_scan) break; if (should_abort_scan(lruvec, sc)) break; cond_resched(); } /* * If too many file cache in the coldest generation can\u0026#39;t be evicted * due to being dirty, wake up the flusher. */ if (sc-\u0026gt;nr.unqueued_dirty \u0026amp;\u0026amp; sc-\u0026gt;nr.unqueued_dirty == sc-\u0026gt;nr.file_taken) wakeup_flusher_threads(WB_REASON_VMSCAN); /* whether this lruvec should be rotated */ return nr_to_scan \u0026lt; 0; } 上面為 MGLRU 中關鍵的程式碼，這是 Memory Reclaim 流程中真正要淘汰 page 的地方\n整個函式可以分成以下部份\n1 int swappiness = get_swappiness(lruvec, sc); 首先取得 swappiness swappiness 控制要回收的 page 傾向，是要傾向於 anon page 還是 file-backed page 如果 swappiness 偏高，表示更傾向於回收 anon page，反之則 file-backed page。這個值會在 get_nr_to_scan 和 evict_folios 中使用，他們會根據 swappiness 決定要處理的 page 類型 1 2 3 4 while (true) { nr_to_scan = get_nr_to_scan(lruvec, sc, swappiness); if (nr_to_scan \u0026lt;= 0) break; 接著進入主要回收迴圈，先計算要掃描的 page 數量 get_nr_to_scan() 根據目前的 memory pressure 和 generation 結構計算目前需要掃描 如果發現結果 \u0026lt;= 0，表示此 lruvec 不需要掃描 1 2 3 delta = evict_folios(lruvec, sc, swappiness); if (!delta) break; 這裡是主要做回收的地方 evict_folios() 是實際掃描並嘗試回收某一層 generation 中的 page (folios) 的函式 delta 表示這次成功處理 (包含 reclaim 以及 skip) 的 page 數量 如果 delta == 0，表示這一層 generation 沒有可以回收的 page，跳出迴圈 1 2 3 scanned += delta; if (scanned \u0026gt;= nr_to_scan) break; 計算已經掃描的 page 數量並確認是否達標 通過 scanned 去累加這次總共掃描了多少 page 如果已經達到預估得掃描數量 nr_to_scan，則退出 1 2 if (should_abort_scan(lruvec, sc)) break; 檢查是否該中止回收行為 內部條件判斷是否要中止，例如 memory pressure 減緩，或 CPU 覆載過高 1 cond_resched(); 讓出 CPU 避免 starvation cond_resched() 允許 scheduler 在這個點做 context switch，避免目前這個函式長時間佔用 CPU 1 2 if (sc-\u0026gt;nr.unqueued_dirty \u0026amp;\u0026amp; sc-\u0026gt;nr.unqueued_dirty == sc-\u0026gt;nr.file_taken) wakeup_flusher_threads(WB_REASON_VMSCAN); 處理 dirty page 的狀況 如果掃描到很多 page 都屬於 dirty file-backed page，且都沒有進入到 flusher queue，就會 wake up flusher thread 避免 dirty page 卡在回收過程中 1 return nr_to_scan \u0026lt; 0; return 並由上層 caller 判斷是否需要進行 rotate 如果 nr_to_scan \u0026lt; 0，表示某些 generation 掃描完畢，應該準備 rotate 到下一個 generation 這個值會回傳到上層如 shrink_one() 決定是否進行 try_to_shrink_lruvec()\n真正執行 LRU page reclaim 的函式，函式目標為是對某個 memory cgroup 的某個 lruvec　嘗試執行 MGLRU 的回收邏輯 caller 會根據目前的 memory pressure 決定要去哪一個 generation 做 reclaim MGLRU 會掃描某一層中 gen X (page vectors)，根據 page 的 hot cold 進行回收 之後通過 evict_folios 檢查後確認要回收 PID 控制迴路與 anon/file-backed page 平衡 MGLRU 在 eviction 決策中引入類似於控制迴路中 PID 控制器的回饋機制，用於平衡 anon page 和 file-backed page 的回收比例。整套機制透過 Refault 率等指標作為回饋訊號來調整 eviction 策略，使得 working set 能夠得到保護。\n回收 page 類型選擇 (P: 比例調節): MGLRU 會比較目前 anon page 與 file-backed page 的 refault 比率，傾向回收 refault 比率較低的 page 類型。如果每一類 page 被回收之後很少被再次存取，也就是很少被 refault，那表示這一類型的 page 對於目前的 working set 不重要，因此把這一些 page 回收掉對性能影響較小，反之，如果某一類型的 page 不斷被 refault，則表示這一類型的 page 包含目前的 working set，應該減少回收這一類型的 page。通過這種方式去平衡 anon page 和 file-backed page 的回收比例 (相比起 2Q LRU 使用 swappiness 固定比例，MGLRU 的實現為動態調整)。\nP 裡面的比率，指的是 refault 率\nworking set 保護 (I: 積分調節): 為了進一步避免某部份 working set 被過度回收，MGLRU 引入 Tier 對 page 的存取頻率進行分類，並基於歷史統計數據對這一些 page 進行保護。每個 page 都有一個 2-bit 的欄位作為 Tier 值，總共分成四個層級。Tier 的計算主要是基於該 page 通過 syscall 如 read(), write() 的存取次數，如果存取次數為 $n$，則 Tier = $\\lfloor \\log_2(n) \\rfloor$，例如如果一個 file-backed page 只被讀取一次，則 Tier=0。被 syscall 讀寫 2 次則 Tier = 1。anon page 通常不使用 Tier 劃分。MGLRU 會持續統計各個 Tier 中 page 被回收和 refault 的次數，從而計算每一 Tier 的 refault 率。在 eviction 過程中，會使用到積分調節的概念，也就是參考一段時間內的歷史資料，如果某些 Tier 較高的 page 歷史 refault 率偏高，即便在某一次掃描時該 page 沒有被引用，MGLRU 也會對該 page 進行 protect，避免一次回收過多此 Tier 的 page。使用過去累積的 refault 訊息調節 eviction 策略。\nD 在 MGLRU 中沒有使用\n總而來說，MGLRU 的 PID 控制迴路將上面的 P 與 I 結合，P 即時的調整回收傾向的 page 類型以及層級，而 I 基於歷史統計對高 refault 率或是高 Tier 的 page 進行保護，P 和 I 共同作用達成保留 working set 的效果。例如某段時間內的 file-backed page 的 refault 率遠高於 anon page，P 會更傾向回收更多 anon page 以保護 working set，也就是保護 file-backed page。同時如果發現某 Tier 的 refault 率教高，則 I 會降低該 Tier page 的回收，必要時會針對個別 page 進行 Protect (移入 min_seq + 1)。MGLRU 對 max_seq 的 anon page 和 file-backed page 始終是同步的，但是對於 min_seq 是不同步的，原因是為了實現上面的 PID 動態平衡機制。傳統 2Q LRU 的第二次存取 page Promote 到 active list 的機制在 MGLRU 中使用 gen 以及上面的 Tier, PID 迴路機制取代，例如對於新映射的 page，一次存取立即放到 max_seq，而對於 syscall 存取的 file-backed page，則需要通過多次存取累積一定的 Tier 才會被 Promote。\n關於 Bloom Filter 降低 Rmap 開銷 MGLRU 在 Aging 階段引入了 Bloom Filter 降低掃描 Page Table 和 Rmap 的開銷。2Q LRU 在判斷 page 是否進行有被使用時，需要對每個 page 進行 Rmap 操作來尋所有映射該 page 的 PTE 並檢查其 Accessed bit，Rmap 需要大量的開銷，為此，Rmap 通過以下方式進行優化\n熱區紀錄: 在 eviction 過程中，當執行 Rmap 檢查 page 的引用時，MGLRU 會同時查看該 page 所在的 PMD 是否包含多個被引用的 page。這部份通過 lru_gen_look_around() 實現，如果 lru_gen_look_around() 在目標 page 的附近 PTE 中發現多個 page 的 Accessed Bit 都是 1，也就是這一些 page 都屬於年輕的，就表示這一整片 PMD 都是 hot page。MGLRU 會將對應的 PMD entry 紀錄到 Bloom Filter 中 (通過 update_bloom_filter() 將 PMD 紀錄到 Bloom Filter) double-buffering 與 flip filters: MGLRU 使用 double-buffering 的 Bloom Filter 實現 flip filter。Bloom Filter 實現的資料結構為 Bit Map 以及 hash function，Bit Map 大小為 $m=2^{15}$，hash function 使用 2 個，可在容納 10000 條 entry 時保持較低的誤判率。每次 Aging 迭代的時候會使用 flip filter 翻轉到另外一個 Bloom Filter，將新發現的熱區紀錄到目前的 Bloom Filter，另一個則清空或是逐步淘汰舊資訊。確保 Bloom Filter 中的內容隨時間更新，裡面的資訊為最近幾次 eviction 所發現的熱區，而不至於保留過時的訊息 Page Table Walker: 在 Aging 流程中，kernel 對每個 process 進行 Page Table 走訪時，會先對每個 PMD 查詢 Bloom Filter (test_bloom_filter())，如果一個 PMD entry 或是其他中間節點在 Bloom Filter 中存在，表示之前 eviction 該區域時裡面有不少 hot page，值得深入走訪。如果 PMD entry 不在 Bloom Filter 中，則該區域可能 hot page 的分佈非常稀疏，大部分都是冷的或是沒有映射，這時候我們就不需要再深入走訪了，可以直接跳過。通過這樣優化減少需要完整掃描的 Page Table 範圍。通過 Bloom Filter 塞選，Page Table Walker 可以跳過那些很可能是冷的區域，專注走訪那些存在較多 Accessed Bit 為 1 的 page。 降低 Rmap 次數: 許多 hot page 會在 Aging 時被標記並在 eviction 時 Promote，而 Bloom Filter 又確保在 Aging 階段不會對整個系統做沒必要的掃描。比起 2Q LRU 依賴 Rmap 一個一個 page 的檢查方式，MGLRU 相當於將部份工作分攤在其他階段處理 在 Eviction 時藉由一次 Rmap 走訪收穫額外週邊多個 page 的 Accessed 訊息 在 Aging 時利用 Eviction 得到的訊息進行掃描 MGLRU 在 Eviction 以及 Aging 之間就建立了一個迴路: Eviction 發現熱區 -\u0026gt; 更新 Bloom Filter -\u0026gt; Aging 根據 Bloom Filter 重點提升 hot page。 藉由上面這種設計，減少不必要的 rmap 操作，減輕記憶體回收對 CPU 的負載。 測試環境 1 2 發行版: Fedora Linux Kernel Version: Linux apt 6.14.0-63.fc42.x86_64 使用 MGLRU debugfs 首先建立 cgroup\n1 $ sudo mkdir -p /sys/fs/cgroup/\u0026lt;name\u0026gt; 接著把指定 process 的 pid 加入到該 cgroup 中\n1 $ echo pid | sudo tee /sys/fs/cgroup/wkgrp/cgroup.procs 可以使用以下指定觀察給定 process 是否加入到指定 cgroup 中\n1 $ systemd-cgls /sys/fs/cgroup/wkgrp 接著執行 workload 觀察 memory.stat，我們可以特別選出 anon page 進行觀察 (malloc 出來的 page　屬於 anon page，我們頻繁的存取這一些 page　這一些 page　屬於 active_anon)\n1 2 3 4 5 6 7 anon 4296462336 anon_thp 0 inactive_anon 36864 active_anon 4296425472 workingset_refault_anon 0 workingset_activate_anon 0 workingset_restore_anon 0 接著我們通過 lru_gen 去控制 page，我們可以通過建立新的 gen 把目前在 active_anon 擠到 inactive_anon 裡面，先查看目前 lru_gen　的資訊\n1 2 3 4 5 6 memcg 133 /wkgrp node 0 14 5208406 9 1 15 5125504 0 0 16 5108911 1048932 0 17 5100847 0 0 可以看到目前 page 都在 young gen 裡面，我們通過以下指令建立新的 gen，我們原先在 young gen 的 page 應該會被擠到 old gen 裡面\n1 echo \u0026#34;+ 133 0 0 0\u0026#34; \u0026gt; /sys/kernel/debug/lru_gen 可以看到我們的 page 進入到了 old gen 裡面\n1 2 3 4 5 6 memcg 133 /wkgrp node 0 16 5159077 302 1 17 5151013 1048622 0 18 33125 17 0 19 21150 0 0 而 old gen 以及 oldest gen 都是屬於 inactive，我們可以在 /sys/fs/cgroup/wkgrp/memory.stat 中看到\n1 2 3 4 5 6 7 anon 4296462336 anon_thp 0 inactive_anon 4296392704 active_anon 69632 workingset_refault_anon 0 workingset_activate_anon 0 workingset_restore_anon 0 我們可以做一個實驗，我們先嘗試存取資料，接著把這一些資料通過 lru_gen　進行 swapout，接著再次進行存取，概念上為以下\n1 2 3 [root@apt]/home/fedora# echo \u0026#34;+ 133 0 0 0\u0026#34; \u0026gt; /sys/kernel/debug/lru_gen [root@apt]/home/fedora# echo \u0026#34;+ 133 0 0 0\u0026#34; \u0026gt; /sys/kernel/debug/lru_gen [root@apt]/home/fedora# echo \u0026#34;- 133 0 45 200\u0026#34; \u0026gt; /sys/kernel/debug/lru_gen 先把我們的 hot page 通過新建 gen 移動到 oldtest gen，接著直接回收最後一個世代，200 表示 swapiness，將其換出到 swapspace 中，我們可以通過 cat /sys/fs/cgroup/wkgrp/memory.swap.current 查看 swapspace 大小，以下為測試程式碼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 #define _GNU_SOURCE #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;stdint.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;time.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define STEP 4096 #define NSEC(ns) ((double)(ns) / 1e9) static uint8_t *cold; static void touch(char *buf, size_t sz) { for (size_t i = 0; i \u0026lt; sz; i += STEP) buf[i]++; /* read-modify-write */ } static uint64_t mono_ns(void) { struct timespec ts; clock_gettime(CLOCK_MONOTONIC_RAW, \u0026amp;ts); return (uint64_t)ts.tv_sec * 1e9 + ts.tv_nsec; } int main(int argc, char **argv) { size_t cold_mib = argc \u0026gt; 1 ? strtoull(argv[1], NULL, 10) : 4096; int wait_sec = argc \u0026gt; 2 ? atoi(argv[2]) : 30; size_t cold_sz = cold_mib * (1UL \u0026lt;\u0026lt; 20); /* bytes */ /* -------- malloc \u0026amp; fault-in -------- */ if (posix_memalign((void **)\u0026amp;cold, 4096, cold_sz)) { perror(\u0026#34;alloc\u0026#34;); return 1; } memset(cold, 0, cold_sz); fprintf(stderr, \u0026#34;PID=%d cold=%zu MiB wait=%d s\\n\u0026#34;, getpid(), cold_mib, wait_sec); /* -------- Phase-1: 首次觸碰 -------- */ touch((char *)cold, cold_sz); fprintf(stderr, \u0026#34;Phase-1 done, cold pages fault-in \u0026amp; active\\n\u0026#34;); /* -------- Phase-2: 等待 mglru 操作 -------- */ fprintf(stderr, \u0026#34;Sleep %d s, you may run mglru_ctl now…\\n\u0026#34;, wait_sec); sleep(wait_sec); /* -------- Phase-3: 重新觸碰 \u0026amp; 量時間 -------- */ uint64_t t0 = mono_ns(); touch((char *)cold, cold_sz); uint64_t t1 = mono_ns(); double sec = NSEC(t1 - t0); printf(\u0026#34;Re-access %.0f MiB took %.3f s (%.1f MiB/s)\\n\u0026#34;, (double)cold_mib, sec, cold_mib / sec); return 0; } 在不做任何調整的情況如下\n1 2 3 4 PID=2696594 cold=8192 MiB wait=10 s Phase-1 done, cold pages fault-in \u0026amp; active Sleep 10 s, you may run mglru_ctl now… Re-access 8192 MiB took 0.046 s (179475.2 MiB/s) 接著我們嘗試將 hot page swap out\n1 2 3 4 PID=2697859 cold=8192 MiB wait=60 s Phase-1 done, cold pages fault-in \u0026amp; active Sleep 60 s, you may run mglru_ctl now… Re-access 8192 MiB took 2.986 s (2743.4 MiB/s) 可以看到效能出現了巨大的下滑，因為這時候是從硬碟中讀取資料，從以上證明了 lru_gen 的可行性\n使用 kprobe 觀測 funciton 行為 如果我們想要觀察 kernel function 內部行為，以 MGLRU 為例子，我們可能想要觀察具體進入 lruvec_folio 的 page 一些資訊，如他所在的 gen 等等去驗證我們的推論是否正確，對於這些情況，我們有許多方式可以實現對 kernel function 的觀察，以下分成靜態與動態的方式\n靜態追蹤 Tracepoint: 為 Linux Kernel 中一種靜態插樁 (static instrumentation) 機制，可以在原始碼中函式部份預先定義 hook points，用來監控與追蹤 kernel 的行為。Tracepoint 在編譯時會嵌入到 kernel 中，可以使用 sudo perf list tracepoint 查看可用的 tracepoint。\n不過缺點也很明顯，對於沒有預先定義 tracepoint 的 function，我們會需要修改並重新編譯 kernel。\n動態追蹤 kprobe: 可以動態的在 kernel funciton 中任意位置插入探針，可以無須重新編譯 kernel，不需要修改 kernel code。\n對於已經在 kallsym 中的 function，我們可以很輕鬆直接使用 symbol name 去 kprobe，但是對於不在 kallsym 的 function，如一些被 compiler 優化掉的 symbol 或是 inline，我們會需要得到該 function 的 function address，具體作法為我們需要 kernel-debuginfo 配合 DWARF 去得出該 function 的地址，這點是從 perf probe 中意外發現的，發現 perf probe -L \u0026lt;function_name\u0026gt; 可以印出 function 的 source code，於是我嘗試以下\n1 2 3 $ sudo perf probe -L try_to_inc_max_seq Failed to find the path for the kernel: No such file or directory Error: Failed to show lines. 出現以上錯誤，嘗試使用 strace 對 perf probe 進行追蹤後發現是依賴於 debug-info，以下為 strace perf probe -L \u0026lt;function_name\u0026gt; 的輸出\n1 2 3 4 - openat(AT_FDCWD, \u0026#34;/usr/lib/debug/usr/lib/debug/lib/modules/6.14.0-63.fc42.x86_64/vmlinux.debug\u0026#34;, O_RDONLY) - openat(AT_FDCWD, \u0026#34;/usr/lib/debug/usr/lib/debug/lib/modules/6.14.0-63.fc42.x86_64/vmlinux\u0026#34;, O_RDONLY) - openat(AT_FDCWD, \u0026#34;/usr/lib/debug/lib/modules/6.14.0-63.fc42.x86_64/.debug/vmlinux\u0026#34;, O_RDONLY) - openat(AT_FDCWD, \u0026#34;/usr/lib/debug/.build-id/a9/1e7e0ceeb1cc106e37bf89b80a69b39844a559.debug\u0026#34;, O_RDONLY) 以 fedora 為例子，通過以下指令我們可以得到 kernel-debuginfo\n1 $ sudo dnf --enablerepo=fedora-debuginfo,updates-debuginfo install kernel-debuginfo 接著再次使用 strace perf probe -L \u0026lt;function_name\u0026gt; 即可正常執行，並且還能夠列出不在 System.map 以及 kallsyms 裡面的 function 資訊，於是我想要通過 debuginfo 去得到那些被優化掉的 function 的記憶體地址\ndebuginfo 會位於 /usr/lib/debug/lib/modules/$(uname -r)/vmlinux 底下，接著我們可以通過以下腳本去得到任一 function 的記憶體地址\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import subprocess import sys import re import os def get_function_start_address(function_name): kernel_release = os.uname().release vmlinux_path = f\u0026#34;/usr/lib/debug/lib/modules/{kernel_release}/vmlinux\u0026#34; print(vmlinux_path) # Using gdb read vmlinux and get function address cmd = [ \u0026#34;gdb\u0026#34;, \u0026#34;-batch\u0026#34;, \u0026#34;-ex\u0026#34;, f\u0026#34;info line {function_name}\u0026#34;, vmlinux_path ] try: output = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True) except subprocess.CalledProcessError as e: print(\u0026#34;GDB execution failed:\u0026#34;) print(e.output) return None match = re.search(r\u0026#34;starts at address ([\\w]+)\u0026#34;, output) if match: return match.group(1) else: print(\u0026#34;Failed to find start address.\u0026#34;) return None if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) != 2: print(f\u0026#34;Usage: sudo python3 {sys.argv[0]} \u0026lt;function_name\u0026gt;\u0026#34;) sys.exit(1) function = sys.argv[1] addr = get_function_start_address(function) if addr: print(f\u0026#34;{function} starts at address: {addr}\u0026#34;) 有了 function address 之後，我們就可以用 kprobe 去探測並觀察其內部資料結構資訊。\n使用 kprobe 有以下兩種方式\n使用 bpftrace 使用 kernel module 使用 kernel module 以 try_to_shrink_lruvec 為例子，如果我們想要得知 lruvec 裡面具體 page 的情況或是 gen 等等，我們可以通過以下 kprobe pre handler 存取 function 內的變數\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 static int handler_pre(struct kprobe *p, struct pt_regs *regs) { struct lruvec *lruvec = NULL; struct lru_gen_folio *lrugen = NULL; int gen, type, zone, page_refcount; lruvec = (struct lruvec *)regs-\u0026gt;di; if (!lruvec) { pr_warn(\u0026#34;lruvec is NULL\\n\u0026#34;); return 0; } lrugen = \u0026amp;lruvec-\u0026gt;lrugen; if (!lrugen) { pr_warn(\u0026#34;lrugen is NULL\\n\u0026#34;); return 0; } pr_info(\u0026#34;[KPROBE] try_to_shrink_lruvec called\\n\u0026#34;); pr_info(\u0026#34;[KPROBE] max_seq: %lu\\n\u0026#34;, lrugen-\u0026gt;max_seq); for (gen = 0; gen \u0026lt; MAX_NR_GENS; gen++) { for (type = 0; type \u0026lt; ANON_AND_FILE; type++) { for (zone = 0; zone \u0026lt; MAX_NR_ZONES; zone++) { struct list_head *head; struct folio *folio; struct page *page; long count = 0; head = \u0026amp;lrugen-\u0026gt;folios[gen][type][zone]; if (!head || list_empty(head)) continue; pr_info(\u0026#34;Generation %d, type %s, zone %d:\\n\u0026#34;, gen, (type == 0 ? \u0026#34;anon\u0026#34; : \u0026#34;file\u0026#34;), zone); list_for_each_entry(folio, head, lru) { if (!folio) { pr_warn(\u0026#34;NULL folio in list\\n\u0026#34;); break; } page = \u0026amp;folio-\u0026gt;page; page_refcount = page_ref_count(page); unsigned long pfn = page_to_pfn(page); unsigned long long page_phys_addr = PFN_PHYS(pfn); bool dirty = PageDirty(page); bool writeback = PageWriteback(page); pr_info(\u0026#34;(folio: %px) (page addr: %px) (page phys addr: %px) (page ref count: %d)\\n\u0026#34;, folio, page, page_phys_addr, page_refcount); } } } } return 0; } 根據 x86 calling convention 我們知道第一個參數位於 rdi 暫存器中，接著按照 MGLRU 關鍵資料結構，由 lruvec 存取 lrugen，接著依照 gen, type, zone 走訪 folio list 便可以得到 page 資訊。\n接著指定我們要 hook 的 function 以及對應的 pre handler 便完成註冊\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 static int __init mglru_monitor_init(void) { int ret; kp.symbol_name = \u0026#34;try_to_shrink_lruvec\u0026#34;; kp.pre_handler = handler_pre; ret = register_kprobe(\u0026amp;kp); if (ret \u0026lt; 0) { pr_err(\u0026#34;register_kprobe failed, returned %d\\n\u0026#34;, ret); return ret; } pr_info(\u0026#34;MGLRU monitor module loaded.\\n\u0026#34;); return 0; } 如果我們要 hook 的 function 的 symbol 在 System.map 中查詢不到，而是使用 Debuginfo 得到的 raw address，則需要使用以下\n1 2 3 4 5 6 7 8 static int __init kprobe_stack_init(void) { kp.addr = (kprobe_opcode_t *)FUNCTION_ADDRESS; kp.pre_handler = handler_pre; ret = register_kprobe(\u0026amp;kp); ... } 成功執行後我們便可以得到具體進入到 try_to_shrink_lruvec() 的 page 相關資訊，得到以下輸出\n1 2 3 4 5 6 7 8 9 10 11 12 $ sudo dmesg [29971.000568] (folio: ffffea001480e480) (page addr: ffffea001480e480) (page phys addr: 0000000520392000) (page ref count: 1) [29971.000569] (folio: ffffea001480e440) (page addr: ffffea001480e440) (page phys addr: 0000000520391000) (page ref count: 1) [29971.000570] (folio: ffffea001480e400) (page addr: ffffea001480e400) (page phys addr: 0000000520390000) (page ref count: 1) [29971.000571] (folio: ffffea001480e3c0) (page addr: ffffea001480e3c0) (page phys addr: 000000052038f000) (page ref count: 1) [29971.000572] (folio: ffffea001480e380) (page addr: ffffea001480e380) (page phys addr: 000000052038e000) (page ref count: 1) [29971.000574] (folio: ffffea001480e340) (page addr: ffffea001480e340) (page phys addr: 000000052038d000) (page ref count: 1) [29971.000575] (folio: ffffea001480e300) (page addr: ffffea001480e300) (page phys addr: 000000052038c000) (page ref count: 1) [29971.000576] (folio: ffffea001480e2c0) (page addr: ffffea001480e2c0) (page phys addr: 000000052038b000) (page ref count: 1) [29971.000577] (folio: ffffea001480e280) (page addr: ffffea001480e280) (page phys addr: 000000052038a000) (page ref count: 1) [29971.000578] (folio: ffffea001480e240) (page addr: ffffea001480e240) (page phys addr: 0000000520389000) (page ref count: 1) ... 使用 bpftrace 可以使用以下 oneline 指令查看給定 function 傳遞的參數\n1 sudo bpftrace -e \u0026#39;kprobe:try_to_shrink_lruvec { $lruvec = (struct lruvec *)arg0; printf(\u0026#34;[KPROBE] try_to_shrink_lruvec called, lruvec: %px, max_seq: %lu\\n\u0026#34;, $lruvec, $lruvec-\u0026gt;lrugen.max_seq); }\u0026#39; 或是直接使用 raw function address (如果 function 不在 kernel symbol table，可以使用 sudo bpftrace -l 查詢)\n1 sudo bpftrace -e \u0026#39;kprobe:0xffffffff81684c0c { printf(\u0026#34;arg0=%lx\\n\u0026#34;, arg0);}\u0026#39; 或是將以下檔案儲存成 .bt，使用 sudo bpftrace 執行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/usr/bin/env bpftrace BEGIN { printf(\u0026#34;Trace try_to_shrink_lruvec function\\n\u0026#34;); printf(\u0026#34;Press Ctrl+C to stop\\n\u0026#34;); } kprobe:try_to_shrink_lruvec { $lruvec = (struct lruvec *)arg0; if ($lruvec == 0) { printf(\u0026#34;[Warning]: lruvec is NULL\\n\u0026#34;); return; } printf(\u0026#34;[KPROBE] try_to_shrink_lruvec been called\\n\u0026#34;); printf(\u0026#34;[KPROBE] lruvec address: %px\\n\u0026#34;, $lruvec); printf(\u0026#34;[KPROBE] timestramp: %lu\\n\u0026#34;, nsecs); printf(\u0026#34;[KPROBE] process PID: %d, process_name: %s\\n\u0026#34;, pid, comm); printf(\u0026#34;[KPROBE] max_seq: %lu\\n\u0026#34;, $lruvec-\u0026gt;lrugen.max_seq); printf(\u0026#34;[KPROBE] call_count: %d\\n\u0026#34;, ++@call_count); printf(\u0026#34;---\\n\u0026#34;); } END { printf(\u0026#34;total call_count: %d\\n\u0026#34;, @call_count); printf(\u0026#34;End\\n\u0026#34;); } 可以看到上面是一個簡單的測試範例，如果我們要對 lruvec 裡面的成員進行遍歷等等，使用 bpftrace 會非常難去實現，因為無法使用變數作為陣列的索引且一些 kernel 中的巨集也無法使用。\n如果只是查看傳遞參數等等，可以快速的使用 bpftrace 進行實現，需要進一步對參數進行解析則使用 Kernel Module 進行實現。\n結論 解釋了 MGLRU 改進以及大致運作流程 運用 Flamegraph/dot-graph 對 kernel 子系統進行分析 運用 kprobe 對 kernel 某個 function 進行精細分析 Reference MGLRU - Yu Zhao Multi-Gen LRU Multi-Gen LRU Kernel.org brendangregg/FlameGraph jrfonseca/gprof2dot Linux Kprobe Kernel Recipes 2017 - Perf in Netflix - Brendan Gregg ","date":"2025-07-20T13:23:54+08:00","image":"https://wuta0209.github.io/p/coscup-2025-lets-tracing-linux-kernel-mglru-%E5%AF%A6%E4%BD%9C%E8%88%87%E5%88%86%E6%9E%90/coscup_hu_c74492ff09b1028b.png","permalink":"https://wuta0209.github.io/p/coscup-2025-lets-tracing-linux-kernel-mglru-%E5%AF%A6%E4%BD%9C%E8%88%87%E5%88%86%E6%9E%90/","title":"Coscup 2025: Let's Tracing Linux Kernel, MGLRU 實作與分析"},{"content":"修改 /etc/environment 以及 /etc/profile 以下為 /etc/environment 內容\n1 2 3 XMODIFIERS=@im=fcitx GTK_IM_MODULE=fcitx QT_IM_MODULE=fcitx 以下為 /etc/profile 內容\n1 2 3 export XMODIFIERS=@im=fcitx export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx 接著只要執行 code --enable-wayland-ime 就可以正常輸入中文了\n使用 alias 我這邊使用的 shell 是 zsh，加上 alias 讓上面的參數永久有效\n1 $ echo \u0026#34;alias code=\u0026#39;code --enable-wayland-ime\u0026#39;\u0026#34; \u0026gt;\u0026gt; ~/.zshrc ","date":"2025-07-20T12:27:11+08:00","permalink":"https://wuta0209.github.io/p/hyprland-vscode-%E7%84%A1%E6%B3%95%E8%BC%B8%E5%85%A5%E4%B8%AD%E6%96%87%E8%A7%A3%E6%B1%BA/","title":"Hyprland VSCode 無法輸入中文解決"}]